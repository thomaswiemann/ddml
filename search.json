[{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://www.thomaswiemann.com/ddml/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://www.thomaswiemann.com/ddml/articles/ddml.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Get Started","text":"article introduction double/debiased machine learning using short-stacking R. Topics discussed include: Estimation single machine learner Estimation multiple machine learners & short-stacking Estimation using different types short-stacking See Articles discussions advanced topics.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/ddml.html","id":"estimation-with-a-single-machine-learner","dir":"Articles","previous_headings":"","what":"Estimation with a Single Machine Learner","title":"Get Started","text":"illustration, apply ddml included random subsample 5,000 observations data Angrist & Evans (1998). data contains information labor supply mothers, children, well demographic data. See ?AE98 details. ddml_late estimates local average treatment effect (LATE) using double/debiased machine learning (see ?ddml_late). high-dimensional nuisance parameters arising estimate LATE conditional expectation functions control variables \\(X\\). particular, require first step estimates reduced forms \\(E[Y|Z=z, X], E[D|Z=z, X]\\) \\(z=0,1\\) \\(E[Z|X]\\). absence functional form assumptions, conditional expectations need estimated nonparametrically. , consider gradient boosting popular xgboost package estimate nuisance parameters. function mdl_xgboost wrapper xgboost, allowing specify parameters original function. See ?mdl_xgboost details take look vignette(\"new_ml_wrapper\") learn write wrapper different machine learner . Double/debiased machine learning relies cross-fitting avoid large bias overfitting estimating nuisance parameters. argument sample_folds = 3 implies 2/3 observations – 3,333 observations – used train machine learner cross-fitting sample fold. (Note estimation based random subsample 5,000 observations. results can thus readily compared Angrist & Evans (1998).)","code":"# Load ddml and set seed library(ddml) set.seed(35611)  # Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")] # Specify a single learner learners_single <- list(what = mdl_xgboost,                         args = list(nrounds = 10,                                     max_depth = 1)) # Estimate the local average treatment effect using xgboost. late_fit <- ddml_late(y, D, Z, X,                       learners = learners_single,                       sample_folds = 3,                       silent = TRUE) summary(late_fit) #> LATE estimation results:  #>   #>     Estimate Std. Error   t value  Pr(>|t|) #>   -0.2680106  0.2449627 -1.094087 0.2739166"},{"path":"https://www.thomaswiemann.com/ddml/articles/ddml.html","id":"estimation-with-multiple-machine-learners-short-stacking","dir":"Articles","previous_headings":"","what":"Estimation with Multiple Machine Learners & Short-Stacking","title":"Get Started","text":"Since statistical properties machine learners depend heavily underlying (unknown!) structure data, adaptive combination multiple machine learners can increase robustness. snippet, ddml_late estimates LATE short-stacking based three base learners: linear regression (see ?ols) lasso (see ?mdl_glmnet) gradient boosting (see ?mdl_xgboost) Short-stacking computationally convenient variant stacking originally introduced Wolpert (1992). Stacking constructs linear combinations base learners minimize --sample mean squared error particular reduced form (e.g., \\(E[Z|X]\\)). Short-stacking uses --sample predictions naturally arise computation double/debiased machine learning estimates due cross-fitting, substantially reduces computational burden (see vignette(\"stacking\")). finite samples, regularizing linear combination base learners constructed via (short-)stacking can improve statistical properties. can specified via ensemble_type argument. , ddml_late estimates nuisance parameters via linear combinations four base learners linear coefficients constrained non-negative sum one. often insightful see base learners contribute final reduced form estimates. snippet shows weights reduced forms \\(E[Y|Z=0,X]\\) \\(E[Y|Z=1,X]\\):","code":"learners_multiple <- list(list(fun = ols),                           list(fun = mdl_glmnet),                           list(fun = mdl_xgboost,                                args = list(nrounds = 10,                                            max_depth = 1))) # Estimate the local average treatment effect using short-stacking with base #     learners ols, lasso, and xgboost. late_fit <- ddml_late(y, D, Z, X,                       learners = learners_multiple,                       ensemble_type = 'nnls1',                       shortstack = TRUE,                       sample_folds = 3,                       silent = TRUE) summary(late_fit) #> LATE estimation results:  #>   #>       Estimate Std. Error    t value  Pr(>|t|) #> nnls1 -0.19173  0.2005266 -0.9561322 0.3390054 cat(\"Stacking weights for E[Y|Z=0, X]: \\n\") #> Stacking weights for E[Y|Z=0, X]: t(late_fit$weights$y_X_Z0) #>           [,1]      [,2]      [,3] #> nnls1 0.228205 0.5782117 0.1935833  cat(\"Stacking weights for E[Y|Z=1, X]: \\n\") #> Stacking weights for E[Y|Z=1, X]: t(late_fit$weights$y_X_Z1) #>            [,1]      [,2]      [,3] #> nnls1 0.2389935 0.6920264 0.0689801"},{"path":"https://www.thomaswiemann.com/ddml/articles/ddml.html","id":"estimation-using-different-types-of-short-stacking","dir":"Articles","previous_headings":"","what":"Estimation using Different Types of Short-Stacking","title":"Get Started","text":"ddml supports multiple schemes constructing linear combinations base learners. Since relies --sample predictions base learners, computationally cheap compute simultaneously. snippet estimates LATE using base learners four different linear combinations: 'nls' constraints coefficients base learner non-negative 'singlebest' selects single MSPE-minimizing base learner 'ols constructs unconstrained linear combinations base learners 'average' computes unweighted average base learners","code":"# Estimate the local average treatment effect using short-stacking with base #     learners ols, lasso, ridge, and xgboost. late_fit <- ddml_late(y, D, Z, X,                       learners = learners_multiple,                       ensemble_type = c('nnls', 'singlebest',                                         'ols', 'average'),                       shortstack = TRUE,                       sample_folds = 3,                       silent = TRUE) summary(late_fit) #> LATE estimation results:  #>   #>              Estimate Std. Error   t value  Pr(>|t|) #> nnls       -0.2432971  0.2120355 -1.147436 0.2512017 #> singlebest -0.2518913  0.1967114 -1.280512 0.2003652 #> ols        -0.2417658  0.2122186 -1.139230 0.2546073 #> average    -0.2117828  0.1958048 -1.081601 0.2794296"},{"path":"https://www.thomaswiemann.com/ddml/articles/ddml.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Get Started","text":"Angrist J, Evans W (1998). “Children Parents’ Labor Supply: Evidence Exogenous Variation Family Size.” American Economic Review, 88(3), 450-477. Wolpert D H (1992). “Stacked generalization.” Neural Networks, 5(2), 241-259.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/did.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Diff-in-Diff with Double/Debiased Machine Learning","text":"article illustrates ddml can complement highly popular package compute group-time average treatment effects conditional parallel trends assumption. result doubly-robust difference--difference estimator staggered treatment adoption designs leverages machine learning (short-)stacking flexibly control covariates – : difference--difference machine learning. excellent introduction differences--differences multiple time periods, see also article. detailed discussion relevant asymptotic theory double/debiased machine learning difference--difference estimators see Chang (2020).","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/did.html","id":"estimation-using-dids-default-estimator","dir":"Articles","previous_headings":"","what":"Estimation using did’s Default Estimator","title":"Diff-in-Diff with Double/Debiased Machine Learning","text":"illustration, consider data Callaway Sant’Anna (2020) county-level teen employment rates 2003-2007 additional details. interested effect treatment log-employment rate lemp assume parallel trends holds conditional county population (lpop log county population). default, group-time average treatment effect estimator package controls linearly additional covariates. (particular, propensity score estimated using logistic regression outcome reduced form estimated via linear regression). code snippet runs default linear specification (similar article). package offers visualization methods using gglpot2: Diff--Diff Estimates. , group-time average treatment effects can easily aggregated, example, estimate dynamic average treatment effects: Dynamic Treatment Effect Estimates.","code":"# Load the did package library(did) set.seed(588239)  # Print the data data(mpdta) head(mpdta) #>     year countyreal     lpop     lemp first.treat treat #> 866 2003       8001 5.896761 8.461469        2007     1 #> 841 2004       8001 5.896761 8.336870        2007     1 #> 842 2005       8001 5.896761 8.340217        2007     1 #> 819 2006       8001 5.896761 8.378161        2007     1 #> 827 2007       8001 5.896761 8.487352        2007     1 #> 937 2003       8019 2.232377 4.997212        2007     1 # Estimate group-time average treatment effects with covariates attgt_lm <- att_gt(yname = \"lemp\",                    gname = \"first.treat\",                    idname = \"countyreal\",                    tname = \"year\",                    xformla = ~lpop,                    data = mpdta)  # summarize the results summary(attgt_lm) #>  #> Call: #> att_gt(yname = \"lemp\", tname = \"year\", idname = \"countyreal\",  #>     gname = \"first.treat\", xformla = ~lpop, data = mpdta) #>  #> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015>  #>  #> Group-Time Average Treatment Effects: #>  Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]   #>   2004 2004  -0.0145     0.0228       -0.0783      0.0492   #>   2004 2005  -0.0764     0.0275       -0.1532      0.0004   #>   2004 2006  -0.1404     0.0350       -0.2381     -0.0428 * #>   2004 2007  -0.1069     0.0325       -0.1976     -0.0162 * #>   2006 2004  -0.0005     0.0232       -0.0652      0.0643   #>   2006 2005  -0.0062     0.0181       -0.0567      0.0443   #>   2006 2006   0.0010     0.0186       -0.0508      0.0527   #>   2006 2007  -0.0413     0.0187       -0.0935      0.0109   #>   2007 2004   0.0267     0.0149       -0.0148      0.0683   #>   2007 2005  -0.0046     0.0155       -0.0479      0.0388   #>   2007 2006  -0.0284     0.0179       -0.0785      0.0216   #>   2007 2007  -0.0288     0.0152       -0.0713      0.0137   #> --- #> Signif. codes: `*' confidence band does not cover 0 #>  #> P-value for pre-test of parallel trends assumption:  0.23267 #> Control Group:  Never Treated,  Anticipation Periods:  0 #> Estimation Method:  Doubly Robust ggdid(attgt_lm, ylim = c(-.4, .4)) # aggregate the group-time average treatment effects dyn_lm <- aggte(attgt_lm, type = \"dynamic\") summary(dyn_lm) #>  #> Call: #> aggte(MP = attgt_lm, type = \"dynamic\") #>  #> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015>  #>  #>  #> Overall summary of ATT's based on event-study/dynamic aggregation:   #>      ATT    Std. Error     [ 95%  Conf. Int.]   #>  -0.0804        0.0192     -0.118     -0.0427 * #>  #>  #> Dynamic Effects: #>  Event time Estimate Std. Error [95% Simult.  Conf. Band]   #>          -3   0.0267     0.0122       -0.0047      0.0582   #>          -2  -0.0036     0.0128       -0.0366      0.0293   #>          -1  -0.0232     0.0157       -0.0637      0.0172   #>           0  -0.0211     0.0119       -0.0519      0.0098   #>           1  -0.0530     0.0164       -0.0953     -0.0107 * #>           2  -0.1404     0.0379       -0.2382     -0.0427 * #>           3  -0.1069     0.0323       -0.1903     -0.0235 * #> --- #> Signif. codes: `*' confidence band does not cover 0 #>  #> Control Group:  Never Treated,  Anticipation Periods:  0 #> Estimation Method:  Doubly Robust ggdid(dyn_lm, ylim = c(-.4, .4))"},{"path":"https://www.thomaswiemann.com/ddml/articles/did.html","id":"contructing-a-xgboost-based-diff-in-diff-estimator","dir":"Articles","previous_headings":"","what":"Contructing a xgboost-based Diff-in-Diff Estimator","title":"Diff-in-Diff with Double/Debiased Machine Learning","text":"Without additional parametric functional form assumptions reduced form equations, guaranteed default att_gt estimator returns convex combination causal effects. linear predictors necessarily correspond conditional expectation functions arising doubly-robust score group-time average treatment effect. resulting misspecification error can lead negative weights aggregation individual-level treatment effects. Fortunately, convex combination causal effects can guaranteed (without parametric functional form assumptions) using machine learning (nonparametric) reduced form estimators. ddml facilitates use large set machine learning reduced form estimators, including simultaneous considerations multiple estimators via (short-)stacking. use ddml estimators package, can make use est_method argument att_gt function (see also ?::att_gt). useful construct method two steps: simple wrapper ddml_att returns objects needed att_gt second wrapper hard-codes arguments passed ddml_att two-step approach allows cleaner code considering multiple ddml-based estimators (article). code-snippet constructs simple estimation method following step 1: potentially suitable machine learning reduced form estimator gradient tree boosting (see also ?mdl_xgboost). code snippet completes second wrapper hard-coding learner arguments. , consider 10-fold cross-fitting heavily regularized gradient tree bosting estimator (eta learning rate, see also ?mdl_xgboost). can now use reduced form estimator my_did_xgboost pass via est_method argument: xgboost-based Diff--Diff Estimates. course, use ddml-based reduced form estimator still allows us leverage various methods package, including construction (visualization ) dynamic average treatment effects: xgboost-based Dynamic Treatment Effect Estimates. gradient tree boosting-based ATT estimate -0.094 slightly larger ATT estimate -0.08 linear estimator package. Given two coefficients, good reason choose one ? ex-ante difficult trade-potential bias misspecification linear estimator suffers potential bias estimation error gradient tree boosting estimator may suffer . ddml allows resolve conflict data-driven manner simultaneous consideration multiple machine learners via (short-)stacking. turn next section.","code":"# load the ddml package library(ddml)  # write a general wrapper for ddml_att ddml_did_method <- function(y1, y0, D, covariates, ...) {   # Compute difference in outcomes   delta_y <- y1 - y0   # Compute the ATT   att_fit <- ddml_att(y = delta_y, D = D, X = covariates, ...)   # Return results   inf.func <- att_fit$psi_b + att_fit$att * att_fit$psi_a   output <- list(ATT = att_fit$att, att.inf.func = inf.func)   return(output) }#DDML_DID_METHOD my_did_xgboost <- function(y1, y0, D, covariates, ...) {   # Hard-code learners   learners = list(what = mdl_xgboost,                   args = list(nround = 500,                               params = list(eta = 0.05, max_depth = 1),                               early_stopping_rounds = 1))   learners_DX = learners    # Call the general ddml_did method w/ additional hard-coded arguments   ddml_did_method(y1, y0, D, covariates,                   learners = learners,                   learners_DX = learners_DX,                   sample_folds = 10,                   silent = TRUE) }#MY_DID_XGBOOST # estimate group-time average treatment effects with ddml attgt_xgboost <- att_gt(yname = \"lemp\",                         gname = \"first.treat\",                         idname = \"countyreal\",                         tname = \"year\",                         xformla = ~lpop,                         data = mpdta,                         est_method = my_did_xgboost)  # summarize the results summary(attgt_xgboost) #>  #> Call: #> att_gt(yname = \"lemp\", tname = \"year\", idname = \"countyreal\",  #>     gname = \"first.treat\", xformla = ~lpop, data = mpdta, est_method = my_did_xgboost) #>  #> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015>  #>  #> Group-Time Average Treatment Effects: #>  Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]   #>   2004 2004  -0.0184     0.0249       -0.0836      0.0467   #>   2004 2005  -0.0791     0.0322       -0.1634      0.0051   #>   2004 2006  -0.1779     0.0519       -0.3136     -0.0422 * #>   2004 2007  -0.1005     0.0389       -0.2024      0.0014   #>   2006 2004  -0.0038     0.0228       -0.0635      0.0559   #>   2006 2005  -0.0022     0.0165       -0.0454      0.0410   #>   2006 2006   0.0014     0.0255       -0.0653      0.0682   #>   2006 2007  -0.0451     0.0218       -0.1021      0.0119   #>   2007 2004   0.0072     0.0306       -0.0729      0.0873   #>   2007 2005   0.0019     0.0204       -0.0516      0.0554   #>   2007 2006  -0.0294     0.0195       -0.0806      0.0217   #>   2007 2007  -0.0557     0.0553       -0.2003      0.0889   #> --- #> Signif. codes: `*' confidence band does not cover 0 #>  #> P-value for pre-test of parallel trends assumption:  0.78716 #> Control Group:  Never Treated,  Anticipation Periods:  0  # plot the coefficients ggdid(attgt_xgboost, ylim = c(-.4, .4)) # aggregate the group-time average treatment effects dyn_xgboost <- aggte(attgt_xgboost, type = \"dynamic\") summary(dyn_xgboost) #>  #> Call: #> aggte(MP = attgt_xgboost, type = \"dynamic\") #>  #> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015>  #>  #>  #> Overall summary of ATT's based on event-study/dynamic aggregation:   #>      ATT    Std. Error     [ 95%  Conf. Int.]   #>  -0.0937        0.0223    -0.1374       -0.05 * #>  #>  #> Dynamic Effects: #>  Event time Estimate Std. Error [95% Simult.  Conf. Band]   #>          -3   0.0072     0.0295       -0.0627      0.0772   #>          -2   0.0006     0.0161       -0.0376      0.0388   #>          -1  -0.0231     0.0159       -0.0606      0.0145   #>           0  -0.0398     0.0388       -0.1319      0.0522   #>           1  -0.0565     0.0179       -0.0988     -0.0141 * #>           2  -0.1779     0.0553       -0.3089     -0.0470 * #>           3  -0.1005     0.0386       -0.1919     -0.0090 * #> --- #> Signif. codes: `*' confidence band does not cover 0 #>  #> Control Group:  Never Treated,  Anticipation Periods:  0 ggdid(dyn_xgboost, ylim = c(-.4, .4))"},{"path":"https://www.thomaswiemann.com/ddml/articles/did.html","id":"contructing-a-shortstacking-based-diff-in-diff-estimator","dir":"Articles","previous_headings":"","what":"Contructing a Shortstacking-based Diff-in-Diff Estimator","title":"Diff-in-Diff with Double/Debiased Machine Learning","text":"Instead considering just single machine learner – may may suitable given application – can leverage (short-)stacking simultaneously consider multiple machine learners. settings, substantially increases robustness underlying structure data. construct new wrapper ddml_did_method hard-codes different reduced form estimators: linear logistic regression gradient tree boosting less regularization random forests less regularization reduced form estimators optimally combined via non-negative least squares. Note specification also includes linear control specifications considered default learner, ensuring machine learners spuriously selected. leverage shortstacking reduce computational time (see also vignette(\"stacking\")). Finally, recompute group-time average treatment effects using shortstacking estimator: Stacking-based Diff--Diff Estimates. results largely similar default linear estimator package, suggesting linear approximation reduced forms sufficiently accurate. (course didn’t know – now, least, can sleep easy!) settings, particular settings multiple control variables, may show starker difference final estimates. Stacking-based Dynamic Treatment Effect Estimates.","code":"my_did_stacking <- function(y1, y0, D, covariates, ...) {   # Hard-code learners for outcome reduced-form   learners = list(list(fun = ols),                   list(fun = mdl_xgboost,                        args = list(nround = 500,                                    params = list(eta = 0.05, max_depth = 1),                                    early_stopping_rounds = 1)),                   list(fun = mdl_xgboost,                        args = list(nround = 500,                                    params = list(eta = 0.1, max_depth = 3),                                    early_stopping_rounds = 1)),                   list(fun = mdl_ranger,                        args = list(num.trees = 1000,                                    max.depth = 1)),                   list(fun = mdl_ranger,                        args = list(num.trees = 1000,                                    max.depth = 20)))   # Hard-code learners for treatment reduced-form   learners_DX = list(list(fun = mdl_glm),                   list(fun = mdl_xgboost,                        args = list(nround = 500,                                    params = list(eta = 0.05, max_depth = 1),                                    early_stopping_rounds = 1)),                   list(fun = mdl_xgboost,                        args = list(nround = 500,                                    params = list(eta = 0.05, max_depth = 3),                                    early_stopping_rounds = 1)),                   list(fun = mdl_ranger,                        args = list(num.trees = 1000,                                    max.depth = 1)),                   list(fun = mdl_ranger,                        args = list(num.trees = 1000,                                    max.depth = 20)))   # Call the general ddml_did method w/ additional hard-coded arguments   ddml_did_method(y1, y0, D, covariates,                   learners = learners,                   learners_DX = learners_DX,                   sample_folds = 10,                   ensemble_type = \"nnls\",                   shortstack = TRUE,                   silent = TRUE) }#MY_DID_STACKING # estimate group-time average treatment effects with ddml attgt_stacking <- att_gt(yname = \"lemp\",                          gname = \"first.treat\",                          idname = \"countyreal\",                          tname = \"year\",                          xformla = ~lpop ,                          data = mpdta,                          est_method = my_did_stacking)  # summarize the results summary(attgt_stacking) #>  #> Call: #> att_gt(yname = \"lemp\", tname = \"year\", idname = \"countyreal\",  #>     gname = \"first.treat\", xformla = ~lpop, data = mpdta, est_method = my_did_stacking) #>  #> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015>  #>  #> Group-Time Average Treatment Effects: #>  Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]   #>   2004 2004  -0.0083     0.0219       -0.0648      0.0482   #>   2004 2005  -0.0781     0.0312       -0.1586      0.0024   #>   2004 2006  -0.1408     0.0404       -0.2450     -0.0365 * #>   2004 2007  -0.1085     0.0355       -0.2003     -0.0168 * #>   2006 2004  -0.0047     0.0231       -0.0643      0.0548   #>   2006 2005  -0.0067     0.0195       -0.0570      0.0436   #>   2006 2006   0.0010     0.0204       -0.0517      0.0537   #>   2006 2007  -0.0417     0.0195       -0.0922      0.0088   #>   2007 2004   0.0275     0.0143       -0.0095      0.0645   #>   2007 2005  -0.0028     0.0174       -0.0478      0.0421   #>   2007 2006  -0.0288     0.0182       -0.0757      0.0182   #>   2007 2007  -0.0284     0.0167       -0.0716      0.0149   #> --- #> Signif. codes: `*' confidence band does not cover 0 #>  #> P-value for pre-test of parallel trends assumption:  0.19468 #> Control Group:  Never Treated,  Anticipation Periods:  0  # plot the coefficients ggdid(attgt_stacking, ylim = c(-.4, .4)) # aggregate the group-time average treatment effects dyn_stacking <- aggte(attgt_stacking, type = \"dynamic\") summary(dyn_stacking) #>  #> Call: #> aggte(MP = attgt_stacking, type = \"dynamic\") #>  #> Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015>  #>  #>  #> Overall summary of ATT's based on event-study/dynamic aggregation:   #>      ATT    Std. Error     [ 95%  Conf. Int.]   #>  -0.0808          0.02    -0.1199     -0.0417 * #>  #>  #> Dynamic Effects: #>  Event time Estimate Std. Error [95% Simult.  Conf. Band]   #>          -3   0.0275     0.0137       -0.0073      0.0624   #>          -2  -0.0033     0.0128       -0.0358      0.0292   #>          -1  -0.0236     0.0154       -0.0628      0.0156   #>           0  -0.0201     0.0118       -0.0500      0.0098   #>           1  -0.0539     0.0169       -0.0967     -0.0110 * #>           2  -0.1408     0.0381       -0.2374     -0.0441 * #>           3  -0.1085     0.0333       -0.1932     -0.0238 * #> --- #> Signif. codes: `*' confidence band does not cover 0 #>  #> Control Group:  Never Treated,  Anticipation Periods:  0 ggdid(dyn_stacking, ylim = c(-.4, .4))"},{"path":"https://www.thomaswiemann.com/ddml/articles/did.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Diff-in-Diff with Double/Debiased Machine Learning","text":"Callaway B, Sant’Anna P (2021). “Difference--differences multiple time periods.” Journal Econometrics, 200-230. Callaway B, Sant’Anna P (2021). “: Difference Differences.” R package version 2.1.2, https://bcallaway11.github.io//. Chang NC (2020). “Double/debiased machine learning difference--difference models.” Econometrics Journal, 177-191.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Example on the Effect of 401k Participation","text":"article illustrates key features ddml estimating effect 401k participation retirement savings. particular, example showcases following features: Estimation using single machine learner Estimation using multiple machine learners via short-stacking Estimation using multiple machine learners different sets control variables Estimation using different sets machine learners continuous binary outcome treatment variables introduction data, article illustrates feature turn.","code":"library(ddml) set.seed(2410072)"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"data-construction","dir":"Articles","previous_headings":"","what":"Data construction","title":"Example on the Effect of 401k Participation","text":"401K data considered Survey Income Program Participation (SIPP) year 1991. use data taken application Chernozhukov et al. (2018). endogenous variable interest indicator equal 1 person participates 401k. interested estimating effect net financial assets control various individual characteristics (e.g., age income).","code":"SIPP91 <- readRDS(\"data/SIPP91.rds\") nobs <- nrow(SIPP91) y <- as.matrix(SIPP91$net_tfa) D <- as.matrix(SIPP91$p401) X <- as.matrix(SIPP91[, c(\"age\", \"inc\", \"educ\", \"fsize\",                       \"marr\", \"twoearn\", \"db\", \"pira\", \"hown\")])"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"estimation-using-a-single-machine-learner","dir":"Articles","previous_headings":"","what":"Estimation using a Single Machine Learner","title":"Example on the Effect of 401k Participation","text":"simplest double/debiased machine learning estimator causal effect 401k participation financial assets considers single machine learner (nonparametrically) control individual characteristics. consider gradient boosting code snippet (see ?ddml_plm ?mdl_xgboost details). comparison simple multiple linear regression estimate indicates difference point estimates one standard deviation, suggesting least non-linearities present data.","code":"# PLM with gradient boosting xgboost_fit <- ddml_plm(y = y, D = D, X = X,                         learners = list(what = mdl_xgboost,                                         args = list(nrounds = 300)),                         sample_folds = 10,                         silent = T) round(summary(xgboost_fit)[2, , 1], 2) #> PLM estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>   13915.72    1603.56       8.68       0.00  # Comparison to multiple linear regression lm_fit <- lm(y ~ D + X) round(summary(lm_fit)$coefficients[2, ], 2) #>   Estimate Std. Error    t value   Pr(>|t|)  #>   11600.89    1345.12       8.62       0.00"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"estimation-using-multiple-machine-learners","dir":"Articles","previous_headings":"","what":"Estimation using Multiple Machine Learners","title":"Example on the Effect of 401k Participation","text":"Instead considering just single – possibly ill-tuned – machine learner control covariates, often helpful combine diverse set learners. code snippet considers set six learners (see ?ols, ?mdl_glmnet, ?mdl_ranger details). learners combined minimize --sample mean square prediction error reduced form outcome endogenous variable controls, respectively. reduce computational burden, consider short-stacking (see also vignette(\"stacking\")). estimate nearly identical estimate using just gradient boosting. better understand machine learners contribute final estimate, can take look assigned stacking weights. row corresponds base-learner (chronological order) columns indicate corresponding reduced form. weights show two random forest estimators contribute reduced forms, followed lasso estimator. Neither gradient boosting linear regression assigned substantial weight.","code":"# Specify set of learners learners <- list(list(fun = ols),                  list(fun = mdl_glmnet),                  list(fun = mdl_ranger,                       args = list(num.trees = 250,                                   max.depth = 4)),                  list(fun = mdl_ranger,                       args = list(num.trees = 250,                                   max.depth = 12)),                  list(fun = mdl_xgboost,                       args = list(nrounds = 100)),                  list(fun = mdl_xgboost,                       args = list(nrounds = 300)))  # PLM with short-stacking stacking_fit <- ddml_plm(y = y, D = D, X = X,                          learners = learners,                          ensemble_type = \"nnls\",                          sample_folds = 10,                          shortstack = T,                          silent = T) round(summary(stacking_fit)[2, , 1], 2) #> PLM estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>   13636.00    1526.01       8.94       0.00 sapply(stacking_fit$weights, round, 4) #>         y_X   D1_X #> [1,] 0.0048 0.0197 #> [2,] 0.2113 0.2817 #> [3,] 0.2973 0.3400 #> [4,] 0.5770 0.3504 #> [5,] 0.0000 0.0000 #> [6,] 0.0000 0.0238"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"estimation-with-different-sets-of-control-variables","dir":"Articles","previous_headings":"","what":"Estimation with Different Sets of Control Variables","title":"Example on the Effect of 401k Participation","text":"large weight assigned random forests suggests non-linearities potential interactions may important. random forests gradient boosting adaptively constructs non-linearities, linear methods lasso need pre-specify variables capture complex patterns data. better set machine learners consider thus consider different sets variables different machine learners: linear regression lasso extended set control variables random forests gradient boosting original set control variables extend control variables using simple third-order polynomial expansion non-binary control variables (interactions). combine original control variables. addition, construct sets indices reference two sets controls. ddml supports passing different sets control variables different base learners via assign_X argument, includes column-indices corresponding desired variables control matrix. assess whether additional effort expanding control variables effect composition reduced form estimates, can inspect stacking weights. Indeed, new base learner combines lasso polynomial expansion controls assigned largest weight, followed random forest estimators.","code":"X_series <- poly(X[, c(\"age\", \"inc\", \"educ\", \"fsize\")], degree = 3) X_c <- cbind(X, X_series) X_baseline <- 1:dim(X)[2] # baseline variables and indicators X_extended <- 5:dim(X_c)[2] # indicators & series expansion # Specify base learners with different sets of controls learners <- list(list(fun = ols,                       assign_X = X_baseline),                  list(fun = ols,                       assign_X = X_extended),                  list(fun = mdl_glmnet,                       assign_X = X_extended),                  list(fun = mdl_ranger,                       args = list(num.trees = 250,                                   max.depth = 4),                       assign_X = X_baseline),                  list(fun = mdl_ranger,                       args = list(num.trees = 250,                                   max.depth = 12),                       assign_X = X_baseline),                  list(fun = mdl_xgboost,                       args = list(nrounds = 100),                       assign_X = X_baseline),                  list(fun = mdl_xgboost,                       args = list(nrounds = 300),                       assign_X = X_baseline))  # PLM with short-stacking stacking_fit <- ddml_plm(y = y, D = D, X = X_c,                          learners = learners,                          ensemble_type = \"nnls\",                          sample_folds = 10,                          shortstack = T,                          silent = T) round(summary(stacking_fit)[2, , 1], 2) #> PLM estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>   14363.22    1500.04       9.58       0.00 sapply(stacking_fit$weights, round, 4) #>         y_X   D1_X #> [1,] 0.0176 0.0000 #> [2,] 0.0000 0.1219 #> [3,] 0.7368 0.4056 #> [4,] 0.0000 0.1693 #> [5,] 0.2702 0.2613 #> [6,] 0.0022 0.0522 #> [7,] 0.0000 0.0048"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"estimation-with-continious-and-binary-outcome-and-treatment-variables","dir":"Articles","previous_headings":"","what":"Estimation with Continious and Binary Outcome and Treatment Variables","title":"Example on the Effect of 401k Participation","text":"Thus far, considered set base learners reduced form outcome controls reduced form endogenous variable controls. contrast net financial assets (approximately) continuous, participating 401k binary indicator. may thus sensible consider reduced form estimators support unit-interval latter reduced form. example, consider logistic regression instead linear regression. Equipped included mdl_glm wrapper generalized linear models (see also ?mdl_glm), now specify second set base-learners reduced form binary endogenous variable interest controls. previous section, consider different sets control variables well. estimating double/debiased machine learning estimator, pass second set base learners via learners_DX argument. weights associated new base learners now slightly larger:","code":"# Specify an additional set of learners for the reduced form E[D|X] learners_DX <- list(list(fun = mdl_glm,                          args = list(family = \"binomial\"),                       assign_X = X_baseline),                  list(fun = mdl_glm,                          args = list(family = \"binomial\"),                       assign_X = X_extended),                  list(fun = mdl_glmnet,                       args = list(family = \"binomial\"),                       assign_X = X_extended),                  list(fun = mdl_ranger,                       args = list(num.trees = 250,                                   max.depth = 4),                       assign_X = X_baseline),                  list(fun = mdl_ranger,                       args = list(num.trees = 250,                                   max.depth = 12),                       assign_X = X_baseline),                  list(fun = mdl_xgboost,                       args = list(nrounds = 100),                       assign_X = X_baseline),                  list(fun = mdl_xgboost,                       args = list(nrounds = 300),                       assign_X = X_baseline))  # PLM with short-stacking and different sets of learners stacking_fit <- ddml_plm(y = y, D = D, X = X_c,                          learners = learners,                          learners_DX = learners_DX,                          ensemble_type = \"nnls\",                          sample_folds = 10,                          shortstack = T,                          silent = T) round(summary(stacking_fit)[2, , 1], 2) #> PLM estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>   14097.13    1483.65       9.50       0.00 sapply(stacking_fit$weights, round, 4) #>         y_X   D1_X #> [1,] 0.0132 0.0627 #> [2,] 0.0000 0.2663 #> [3,] 0.7411 0.0086 #> [4,] 0.0000 0.4221 #> [5,] 0.2690 0.2552 #> [6,] 0.0000 0.0000 #> [7,] 0.0000 0.0302"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"bonus-instrumental-variable-estimation","dir":"Articles","previous_headings":"","what":"Bonus: Instrumental Variable Estimation","title":"Example on the Effect of 401k Participation","text":"Instead considering partially linear regression model, may also interested instrumental variable approach. commonly used instrument participation 401k 401k eligibility. code-snippet estimates partially linear IV model using two sets machine learners introduced previous sections. instrument binary, use generalized linear models estimate reduced form 401k eligibility controls.","code":"# 401k eligibility as the instrument Z <- as.matrix(SIPP91$e401)  # PLIV with short-stacking and different sets of learners stacking_IV_fit <- ddml_pliv(y = y, D = D, Z = Z, X = X_c,                          learners = learners,                          learners_DX = learners_DX,                          learners_ZX = learners_DX,                          ensemble_type = \"nnls\",                          sample_folds = 10,                          shortstack = T,                          silent = T) round(summary(stacking_IV_fit)[2, , 1], 2) #> PLIV estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>   12986.88    1871.81       6.94       0.00  # Stacking weights associated with each base learner sapply(stacking_IV_fit$weights, round, 4) #>         y_X   D1_X   Z1_X #> [1,] 0.0000 0.0647 0.0000 #> [2,] 0.0000 0.2271 0.2414 #> [3,] 0.7474 0.0080 0.0157 #> [4,] 0.0000 0.4030 0.5337 #> [5,] 0.2953 0.3036 0.2501 #> [6,] 0.0000 0.0351 0.0000 #> [7,] 0.0000 0.0000 0.0000"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_401k.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Example on the Effect of 401k Participation","text":"Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). “Double/debiased machine learning treatment structural parameters.” Econometrics Journal, 21(1), C1-C68.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"article revisits empirical example Chernozhukov, Hansen, Spindler (2015) (CHS2015, hereafter), extends instruments Berry, Levinsohn, Pakes (1995) (BLP1995, hereafter) applies instrument selection procedure based lasso. consider instrument extension apply double/debiased machine learning short-stacking combines conventional linear estimators computational alternatives including lasso-based approaches, random forests, gradient boosting.","code":"library(ddml) library(AER) # for iv regression set.seed(713954)"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"data-construction","dir":"Articles","previous_headings":"","what":"Data Construction","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"consider automobile market data Berry, Levinsohn, Pakes (1995) retrieved reproduction exercise Chernozhukov, Hansen, Spindler (2015). link snippet constructs BLP1995 instruments, sums product characteristics (excluding price potentially endogenous variables) products offered firm well competing firms. exact instrument specification follows approach CHS2015. addition, may interested extending list instruments. constructs additional instruments considered CHS2015.","code":"BLP95 <- readRDS(\"data/BLP95.rds\")  nobs <- length(BLP95$id) y <- as.matrix(log(BLP95$share) -  log(BLP95$outshr)) D <- as.matrix(BLP95$price) X <- as.matrix(cbind(1, BLP95[, c(\"hpwt\", \"air\", \"mpd\", \"space\")])) colnames(y) <- \"share\" colnames(D) <- \"price\" ncol_X <- ncol(X) sum_other <- sum_rival <- matrix(0, nobs, 5) for (i in 1:nobs) {   other_ind <- (BLP95$firmid == BLP95$firmid[i]) &     (BLP95$cdid == BLP95$cdid[i]) & (BLP95$id != BLP95$id[i])   rival_ind <- (BLP95$firmid != BLP95$firmid[i]) &     (BLP95$cdid == BLP95$cdid[i])   sum_other[i, ] <- colSums(X[other_ind, , drop = FALSE])   sum_rival[i, ] <- colSums(X[rival_ind, , drop = FALSE]) }#FOR Z <- cbind(sum_other, sum_rival); ncol_Z <- ncol(Z) cat(\"Number of baseline controls: \", ncol_X, \"\\n\",     \"Number of baseline instruments: \", ncol_Z) #> Number of baseline controls:  5  #>  Number of baseline instruments:  10 tu = BLP95$trend/19; mpdu = BLP95$mpd/7; spaceu = BLP95$space/2; XL <- as.matrix(cbind(1, BLP95[, c(\"hpwt\", \"air\")], mpdu, spaceu, tu,                       BLP95$hpwt^2, BLP95$hpwt^3, mpdu^2, mpdu^3,                       spaceu^2, spaceu^3, tu^2, tu^3, BLP95$hpwt *                         BLP95$air,  mpdu * BLP95$air, spaceu *                         BLP95$air, tu * BLP95$air, BLP95$hpwt *                         mpdu, BLP95$hpwt * spaceu, BLP95$hpwt * tu,                       mpdu * spaceu,  mpdu * tu, spaceu * tu)) ncol_XL <- ncol(XL) sum_otherL <- sum_rivalL <- matrix(0, nobs, 24) for (i in 1:nobs) {   other_ind <- (BLP95$firmid == BLP95$firmid[i]) &     (BLP95$cdid == BLP95$cdid[i]) & (BLP95$id != BLP95$id[i])   rival_ind <- (BLP95$firmid != BLP95$firmid[i]) &     (BLP95$cdid == BLP95$cdid[i])   sum_otherL[i, ] <- colSums(XL[other_ind, , drop = FALSE])   sum_rivalL[i, ] <- colSums(XL[rival_ind, , drop = FALSE]) }#FOR ZL <- cbind(sum_otherL,sum_rivalL); ncol_ZL <- ncol(ZL) cat(\"Number of extended controls: \", ncol_XL, \"\\n\",     \"Number of extended instruments: \", ncol_ZL) #> Number of extended controls:  24  #>  Number of extended instruments:  48"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"baseline-estimates","dir":"Articles","previous_headings":"","what":"Baseline Estimates","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"begin computing OLS TSLS estimates using baseline controls instrumental variables. Note TSLS estimates differ CHS2015. due slight instrument-construction error original code CHS2015. Similarly, compute estimates using expanded set controls instruments.","code":"ols_fit <- lm(y ~ D + X) round(summary(ols_fit)$coefficients[2, ], 4) #>   Estimate Std. Error    t value   Pr(>|t|)  #>    -0.0886     0.0040   -22.0145     0.0000  tsls_fit <- ivreg(y ~ D + X | X + Z) round(summary(tsls_fit)$coefficients[2, ], 4) #>   Estimate Std. Error    t value   Pr(>|t|)  #>    -0.1357     0.0108   -12.5993     0.0000 ols_L_fit <- lm(y ~ D + XL) round(summary(ols_L_fit)$coefficients[2, ], 4) #>   Estimate Std. Error    t value   Pr(>|t|)  #>    -0.0991     0.0044   -22.4630     0.0000  tsls_L_fit <- ivreg(y ~ D + XL | XL + ZL) round(summary(tsls_L_fit)$coefficients[2, ], 4) #>   Estimate Std. Error    t value   Pr(>|t|)  #>    -0.1258     0.0070   -17.8691     0.0000"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"estimating-the-flexible-partially-linear-iv-model-with-cv-lasso","dir":"Articles","previous_headings":"","what":"Estimating the Flexible Partially Linear IV Model with CV-Lasso","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"Given large set controls instruments expanded set relative moderate sample size, reasonable consider regularized estimators. frequent choice many variables lasso-based estimators. , combine double/debiased machine learning lasso selection instruments controls. (See ?mdl_glmnet ?ddml_fpliv details.)","code":"# Base learner learner <- list(what = mdl_glmnet)  # Estimate the ddml estimator using a single base learner lasso_fit <- ddml_fpliv(y, D = D,                         Z = ZL, X = XL,                         learners = learner,                         sample_folds = 10,                         silent = T) round(summary(lasso_fit)[2, , 1], 4) #> FPLIV estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>    -0.1473     0.0090   -16.3142     0.0000"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"estimating-the-flexible-partially-linear-iv-model-with-multiple-learners","dir":"Articles","previous_headings":"","what":"Estimating the Flexible Partially Linear IV Model with Multiple Learners","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"ambitious approach may consider variety computational models estimation first second stage. , consider combination linear models, including unpenalized regression (see ?ols), lasso ridge regression, random forests (see ?mdl_ranger), boosted trees (see ?mdl_xgboost). addition allowing different machine learners, also consider different sets instruments controls: Either initial set instruments BLP1995 extended set CHS2015. purpose, convenient pre-specify sets indices corresponding initial extended sets. Interestingly, coefficient closer OLS estimates TSLS estimates (without lasso)! better understand composition final estimator, often useful inspect stacking weights. may readily retrieved fitted object. , see boosted trees random forest learners assigned weight cross validation informed ensemble procedures. linear methods – ols, lasso, ridge – contribute substantially final estimates, suggesting user-defined expansions controls instruments little improve bias precision.","code":"# Construct column indices for combined control and instrument sets X_c <- cbind(X, XL); colnames(X_c) <- c(1:ncol(X_c)) Z_c <- cbind(Z, ZL); colnames(Z_c) <- c(1:ncol(Z_c)) set_X <- 1:ncol(X); set_XL <- setdiff(c(1:ncol(X_c)), set_X) set_Z <- 1:ncol(Z); set_ZL <- setdiff(c(1:ncol(Z_c)), set_Z)  # Base learners learners <- list(list(fun = ols, # ols with the baseline set                       assign_X = set_X,                       assign_Z = set_Z),                  list(fun = ols, # ols with the extended set                       assign_X = set_XL,                       assign_Z = set_ZL),                  list(fun = mdl_glmnet, # lasso with the extended set                       args = list(alpha = 1),                       assign_X = set_XL,                       assign_Z = set_ZL),                  list(fun = mdl_glmnet, # ridge with the extended set                       args = list(alpha = 0),                       assign_X = set_XL,                       assign_Z = set_ZL),                  list(fun = mdl_ranger, # random forests with the baseline set                       args = list(num.trees = 1000,                                   min.node.size = 10),                       assign_X = set_X,                       assign_Z = set_Z),                  list(fun = mdl_xgboost, # boosted trees with the baseline set                       args = list(nrounds = 300),                       assign_X = set_X,                       assign_Z = set_Z))  # Compute short-stacked IV estimate stacking_fit <- ddml_fpliv(y, D = D,                            Z = Z_c, X = X_c,                            learners = learners,                            ensemble_type = c(\"nnls1\"),                            shortstack = T,                            sample_folds = 10,                            silent = T) t(round(summary(stacking_fit), 4)[2, , ]) #> FPLIV estimation results:  #>  #>      Estimate Std. Error  t value Pr(>|t|) #> [1,]  -0.0982     0.0092 -10.7008        0 sapply(stacking_fit$weights, round, 4) #>         y_X D1_X  D1_XZ #> [1,] 0.0000 0.00 0.0000 #> [2,] 0.0000 0.00 0.0000 #> [3,] 0.0000 0.00 0.0000 #> [4,] 0.0000 0.00 0.0000 #> [5,] 0.4394 0.45 0.3804 #> [6,] 0.5606 0.55 0.6196"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"elasticities","dir":"Articles","previous_headings":"","what":"Elasticities","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"Equipped multiple coefficient estimates, may recalculate number products inelastic demand CHS2015. BLP1995’s TSLS estimates suggest 746-896 products inelastic demand, nearly half number products implied simple OLS estimates. Double/debiased machine learning estimates using single lasso base learner suggest smallest number inelastic products. stark contrast, estimates based multiple machine learners suggest number closer intial OLS estimates.","code":"compute_inelastic_demand <- function(price_coef) {   sum(price_coef * (BLP95$price) * (1 - BLP95$share) > -1) }#COMPUTE_INELASTIC_DEMAND # OLS implied number of products with inelastic demand compute_inelastic_demand(ols_fit$coef[2]) #> [1] 1502 compute_inelastic_demand(ols_L_fit$coef[2]) #> [1] 1405  # TSLS implied number of products with inelastic demand compute_inelastic_demand(tsls_fit$coef[2]) #> [1] 746 compute_inelastic_demand(tsls_L_fit$coef[2]) #> [1] 896 # ddml-lasso implied number of products with inelastic demand compute_inelastic_demand(lasso_fit$coef) #> [1] 596  # ddml-stacking implied number of products with inelastic demand compute_inelastic_demand(stacking_fit$coef) #> [1] 1417"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"bonus-post-lasso-estimates-without-sample-splitting","dir":"Articles","previous_headings":"","what":"Bonus: Post-Lasso Estimates without Sample-Splitting","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"addition ddml, can also consider estimators hdm package based rigorous lasso – .e., plug-penalty parameters without sample-splitting. coefficient drastically different previous estimates, including double/debaised machine learning estimates included lasso-based approaches. gain insight potential causes differences, check instruments controls selected lasso procedure. surprisingly, lasso plug-penalty selects instruments controls. stacking weights , know double/debaised machine learning estimator assigns weight boosted trees. contrast lasso-based estimates, boosted trees adaptively create interactions input variables, allowing rich non-linearities final predictions. thus makes sense check whether stark differences rlasso-based IV estimates stacking estimates primarily due potential non-linearities opposed specific instrument control variables selected. code snippet re-estimates stacking learner pre-selected set controls instruments. similarity coefficient estimates initial stacking estimates suggest seems adaptively created interactions indeed key driver coefficient differences. confirmed stacking weights, place substantial weight boosted trees. example thus highlights importance considering multiple machine learners robustness illustrates usefulness double/debiased machine learning stacking practical solution.","code":"# Load hdm library(hdm)  # Compute post-lasso IV estimator with the plug-in penalty rlassoIV_fit <- rlassoIV(x = XL, d = D, y = y, z = ZL,                          select.Z = TRUE, select.X = FALSE, post = TRUE) summary(rlassoIV_fit) #> [1] \"Estimates and significance testing of the effect of target variables in the IV regression model\" #>        coeff.      se. t-value  p-value     #> [1,] -0.31558  0.05354  -5.894 3.78e-09 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Zr_ <- ZL[, which(rlassoIV_fit$selected[1:48])] dim(Zr_)[2] #> [1] 2 Xr_ <- XL[, which(rlassoIV_fit$selected[49:length(rlassoIV_fit$selected)])] dim(Xr_)[2] #> [1] 6 # Base learner learner <- list(list(fun = ols),                 list(fun = mdl_xgboost,                      args = list(nrounds = 300)))  # Compute short-stacked IV estimate stacking_r_fit <- ddml_fpliv(y, D = D,                         Z = Zr_, X = Xr_,                         learners = learner,                         ensemble_type = c(\"nnls1\"),                         shortstack = T,                         sample_folds = 10,                         silent = T) round(summary(stacking_r_fit)[2, , 1], 4) #> FPLIV estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>    -0.1044     0.0128    -8.1682     0.0000 sapply(stacking_r_fit$weights, round, 4) #>       y_X   D1_X  D1_XZ #> [1,] 0.08 0.1287 0.0439 #> [2,] 0.92 0.8713 0.9561"},{"path":"https://www.thomaswiemann.com/ddml/articles/example_BLP95.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Example based on Berry, Levinson, & Pakes (1995)","text":"Berry S, Levinshon J, Pakes (1995). “Automobile Prices Market Equilibrium.” Econometrica, 63(4), 841-890. Chernozhukov V, Hansen C, Spindler M (2015). “Post-selection post-regularization inference linear models many controls instruments.” American Economic Review, 105(5), 486-490.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/new_ml_wrapper.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Constructing a User-Provided Base Learner","text":"article illustrates users can write base learners. necessary requirements base learner compatible ddml minimal: estimation function takes outcome vector y feature matrix X returns fitted model object S3 class predict method S3 class takes feature matrix newdata returns numerical vector predictions outline requirements two concrete examples: simple wrapper gbm::gbm.fit() generalized boosted regression extensive wrapper neural networks based keras test wrappers, apply included random subsample 5,000 observations data Angrist & Evans (1998). See ?AE98 details.","code":"# Load ddml and set seed library(ddml) set.seed(75039)  # Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]"},{"path":"https://www.thomaswiemann.com/ddml/articles/new_ml_wrapper.html","id":"a-wrapper-for-gbmgbm-fit","dir":"Articles","previous_headings":"","what":"A Wrapper for gbm::gbm.fit()","title":"Constructing a User-Provided Base Learner","text":"begin writing estimation function takes named arguments y X returns fitted model object. allow arguments passed wrapper gbm::gbm.fit(), also make use ... argument. Note default, predicted values gbm::predict.gbm() return values scale outcome variable (see ?gbm::predict.gbm()). therefore important also construct simple prediction method mdl_gbm object: simple test, check class object returned wrapper prediction method constructed: classes expected. wrapper now compatible ddml machinery. final test, estimate local average treatment effect. (Computation can take , feel free set n.trees interaction.depth smaller value test.) works well!","code":"# Load gbm library(gbm) #> Loaded gbm 2.1.8.1  # Write gbm wrapper mdl_gbm <- function(y, X, ...) {   gbm_fit <- gbm.fit(x = X, y = y, ...) # fit gbm   class(gbm_fit) <- c(\"mdl_gbm\", class(gbm_fit)) # append class   return(gbm_fit) # return fitted gbm object }#MDL_GBM # Write prediction method for gbm.wrapper predict.mdl_gbm <- function(object, newdata, ...) {   class(object) <- class(object)[-1] # remove mdl_gbm from the class list   predict(object, newdata, type = \"response\", n.trees = object$n.trees) }#MDL_GBM # Estimate gbm gbm_fit <- mdl_gbm(y, X,                     distribution  = \"bernoulli\",                    n.trees = 1000,                    interaction.depth = 6,                     verbose = FALSE) # Compute predictions fitted_values <- predict(gbm_fit, X) # Check class class(gbm_fit) #> [1] \"mdl_gbm\" \"gbm\" class(fitted_values) #> [1] \"numeric\" # Use the new gbm base learner learner_gbm <-  list(what = mdl_gbm,                      args = list(distribution  = \"bernoulli\",                                  n.trees = 1000,                                  interaction.depth = 6,                                   verbose = FALSE))  # Estimate ddml_late late_fit <- ddml_late(y, D, Z, X,                       learners = learner_gbm,                       sample_folds = 10,                       silent = TRUE) summary(late_fit) #> LATE estimation results:  #>   #>     Estimate Std. Error   t value  Pr(>|t|) #>   -0.2330304   0.204703 -1.138383 0.2549607"},{"path":"https://www.thomaswiemann.com/ddml/articles/new_ml_wrapper.html","id":"a-wrapper-for-neural-networks-based-on-keras","dir":"Articles","previous_headings":"","what":"A Wrapper for Neural Networks based on keras","title":"Constructing a User-Provided Base Learner","text":"now consider example using neural network implementation keras package. See guide keras basics excellent introduction neural networks R. requirements wrapper compatible ddml , however, estimating neural network involved, need write extensive wrapper. particular, wrapper keras package four components: Building neural network architecture Compiling neural network object Estimating neural network Appending class returned object (write custom predict method) wrapper allows custom specification feed-forward neural network architecture using relu activation function. course, much involved architectures also supported (feel free experiment!). prediction method fitted object ensures predictions returned numerical vector: test wrapper, estimate local average treatment effect. addition architecture, important properly tune optimization algorithm. callback_list helps specifying learning rate adjustments defines early stopping rule. works well!","code":"# Load keras library(keras) mdl_keras <- function(y, X,                       units = 10, nhidden = 1,                       optimizer_fun = \"rmsprop\",                       loss = \"mse\",                       epochs = 10,                       batch_size = min(1000, length(y)),                       validation_split = 0,                       callbacks = NULL,                       steps_per_epoch = NULL,                       metrics = c(\"mae\"),                       verbose = 0) {    # ===============================================   # ADJUST THIS PART FOR DIFFERENT ARCHITECTURES ==    # Construct neural network architecture   nnet <- keras_model_sequential()   for (k in 1:nhidden) {     nnet <- nnet %>%       layer_dense(units = units, use_bias = T,                   activation = \"relu\")   }#FOR   nnet <- nnet %>%     layer_dense(units = 1, use_bias = T)    # ===============================================   # ===============================================    # Compile neural net   nnet %>% keras::compile(optimizer = optimizer_fun,                            loss = loss,                            metrics = metrics)    # Fit neural net   nnet %>% keras::fit(X, y,                        epochs = epochs,                        batch_size = batch_size,                        validation_split = validation_split,                        callbacks = callbacks,                        steps_per_epoch = steps_per_epoch,                        verbose = verbose)    # Append class   class(nnet) <- c(\"mdl_keras\", class(nnet))    # Return fitted object   return(nnet) }#MDL_KERAS predict.mdl_keras <- function(object, newdata){   class(object) <- class(object)[-1] # Remove custom class from object   as.numeric(predict(object, newdata)) }#PREDICT.MDL_KERAS # Specify callbacks callbacks_list <- list(callback_early_stopping(monitor = \"val_loss\",                                                patience = 15,                                                restore_best_weights = T),                        callback_reduce_lr_on_plateau(monitor = \"val_loss\",                                                      factor = 1/10,                                                      patience = 10,                                                      verbose = F))  # Use the neural network base learner learner_keras = list(what = mdl_keras,                      args = list(units = 10, nhidden = 1,                                  epochs = 100,                                  verbose = F,                                  validation_split = 0.1,                                  callbacks = callbacks_list))  # Estimate ddml_late late_fit <- ddml_late(y, D, Z, X,                       learners = learner_keras,                       sample_folds = 10,                       silent = T) summary(late_fit) #> LATE estimation results:  #>   #>     Estimate Std. Error   t value    Pr(>|t|) #>   -0.6972909  0.2566123 -2.717293 0.006581826"},{"path":"https://www.thomaswiemann.com/ddml/articles/sparse.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Estimation with Sparse Matrices","text":"ddml supports sparse matrices Matrix package default. article illustrates double/debiased machine learning estimation sparse matrices using prominent study Angrist Krueger (1991) (AK91, hereafter) returns education.","code":"library(ddml) library(Matrix) # for sparse matrix operations set.seed(900837)"},{"path":"https://www.thomaswiemann.com/ddml/articles/sparse.html","id":"data-construction","dir":"Articles","previous_headings":"","what":"Data Construction","title":"Estimation with Sparse Matrices","text":"One coefficients interest AK91 effect years education log weekly wage American males born 1930-1939. authors instrument years schooling quarter birth indicators (QOB). strategy motivated mandatory attendance laws determine age children may drop school. Since children born later quarters achieve age cut-years schooling, higher QOB imply years schooling. Although data quite large (\\(n = 329509\\)), need sparse matrices arises QOB instrument interacted variables. Popular control variables place birth (POB) year birth (YOB). Depending whether separately jointly interacted QOB, results 180 1530 instruments, respectively. code snippet constructs two sets instruments well matrix controls. use sparse.model.matrix() construct sparse matrix objects supported Matrix package. illustrate need sparse matrices working data AK91, consider differences memory needed hold control matrix alone. \\(329509 \\times 510\\) matrix control variables, sparse version requires 34.3Mb dense version requires 1.3Gb(=1300Mb)! instrument matrix 1530 instrument requires even space regularly fails load memory.","code":"# Load data AK91 <- readRDS(\"data/AK91.rds\")  # Obtain instument matrix for 180 IVs dat_IV180 <- sparse.model.matrix(~ YOB*POB +  QOB*YOB + QOB*POB, data = AK91) which_instrument <- which(grepl(\"QOB\", colnames(dat_IV180), fixed = TRUE)) Z_IV180 <- dat_IV180[, which_instrument]  # Obtain instument matrix for 1530 IVs dat_IV1530 <- sparse.model.matrix(~ QOB * YOB * POB, data = AK91) which_instrument <-  which(grepl(\"QOB\", colnames(dat_IV1530), fixed = TRUE)) Z_IV1530 <- dat_IV1530[, which_instrument]  # Obtain control matrix X <- dat_IV1530[, -which_instrument] # Memory needed for the sparse control matrix format(object.size(X), units = \"Mb\") #> [1] \"34.3 Mb\"  # Memory needed for the dense control matrix format(object.size(as.matrix(X)), units = \"Mb\") #> [1] \"1302.3 Mb\""},{"path":"https://www.thomaswiemann.com/ddml/articles/sparse.html","id":"estimation-with-sparse-matrices","dir":"Articles","previous_headings":"","what":"Estimation with Sparse Matrices","title":"Estimation with Sparse Matrices","text":"syntax estimation sparse matrices ddml exactly estimation dense matrices. , replicate AK91 using simplified set instruments controls. begin estimating returns schooling using set 180 instruments. Following convention returns education-literature, estimator selects among instruments regularize coefficients corresponding control variables. achieved formulating different base learners first second stage reduced forms (see ?ddml_fpliv), setting penalty.factor control variables zero (see ?mdl_glmnet). exercise can repeated larger set 1530 instruments well. Without support sparse matrices, estimation step possible without large memory. coefficients corresponding two sets instruments quite different. Leveraging ddml functionality allows specification different sets input variables, construct stacking estimator considers first stage regressions simultaneously. resulting coefficient close coefficient based 180 instruments. stacking weights confirm first stage estimators 180 instruments contribute almost exclusively final estimate, suggesting expansion 1530 instrument little benefit ols lasso-based first stage fits.","code":"learners_XZ <- list(list(fun = ols),                     list(fun = mdl_glmnet,                          args = list(cv = FALSE,                                      penalty.factor = c(rep(0, 510),                                                         rep(1, 180)))))  stacking_180IV_fit <- ddml_fpliv(y = AK91$LWKLYWGE, D = AK91$EDUC,                                  Z = Z_IV180, X = X,                                  learners = list(list(fun = ols)),                                  learners_DX = list(list(fun = ols)),                                  learners_DXZ = learners_XZ,                                  ensemble_type = c(\"nnls1\"),                                  shortstack = T,                                  sample_folds = 2,                                  silent = T) round(summary(stacking_180IV_fit, type = 'HC1'), 4)[2, , 1] #> FPLIV estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>     0.1121     0.0215     5.2258     0.0000 learners_XZ <- list(list(fun = ols),                     list(fun = mdl_glmnet,                          args = list(cv = FALSE,                                      penalty.factor = c(rep(0, 510),                                                         rep(1, 1530)))))  stacking_1530IV_fit <- ddml_fpliv(y = AK91$LWKLYWGE, D = AK91$EDUC,                                  Z = Z_IV1530, X = X,                                  learners = list(list(fun = ols)),                                  learners_DX = list(list(fun = ols)),                                  learners_DXZ = learners_XZ,                                  ensemble_type = c(\"nnls1\"),                                  shortstack = T,                                  sample_folds = 2,                                  silent = T) round(summary(stacking_1530IV_fit, type = 'HC1'), 4)[2, , 1] #> FPLIV estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>     0.0630     0.0357     1.7636     0.0778 # Construct column indices for combined control and instrument sets Z_c <- cbind(Z_IV180, Z_IV1530); colnames(Z_c) <- 1:(180 + 1530) set_IV180 <- 1:180; set_IV1530 <- 181:(180 + 1530)  learners_XZ <- list(list(fun = ols,                          assign_Z = set_IV180),                     list(fun = ols,                           assign_Z = set_IV1530),                     list(fun = mdl_glmnet,                          args = list(cv = FALSE,                                      penalty.factor = c(rep(0, 510),                                                         rep(1, 180))),                           assign_Z = set_IV180),                     list(fun = mdl_glmnet,                          args = list(cv = FALSE,                                      penalty.factor = c(rep(0, 510),                                                         rep(1, 1530))),                          assign_Z = set_IV1530)) stacking_fit <- ddml_fpliv(y = AK91$LWKLYWGE, D = AK91$EDUC,                            Z = Z_c, X = X,                            learners = list(list(fun = ols)),                            learners_DX = list(list(fun = ols)),                            learners_DXZ = learners_XZ,                            ensemble_type = c(\"nnls1\"),                            shortstack = T,                            sample_folds = 2,                            silent = T) round(summary(stacking_fit, type = 'HC1'), 4)[2, , 1] #> FPLIV estimation results:  #>  #>   Estimate Std. Error    t value   Pr(>|t|)  #>     0.1241     0.0201     6.1669     0.0000 round(stacking_fit$weights$D1_XZ, 4) #>       nnls1 #> [1,] 0.7000 #> [2,] 0.0000 #> [3,] 0.2925 #> [4,] 0.0075"},{"path":"https://www.thomaswiemann.com/ddml/articles/sparse.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Estimation with Sparse Matrices","text":"Angrist J, Krueger (1991). “Compulsory School Attendance Affect Schooling Earnings?” Quarterly Journal Economics, 106(4), 979-1014.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/stacking.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Computational Benefits of Short-Stacking","text":"article illustrates computational advantages short-stacking conventional stacking estimation structural parameters using double/debiased machine learning. See also Ahrens et al. (2023, 2024) discussion short-stacking.","code":""},{"path":"https://www.thomaswiemann.com/ddml/articles/stacking.html","id":"estimation-with-stacking-and-short-stacking","dir":"Articles","previous_headings":"","what":"Estimation with Stacking and Short-Stacking","title":"Computational Benefits of Short-Stacking","text":"apply ddml included random subsample 5,000 observations data Angrist & Evans (1998). data contains information labor supply mothers, children, well demographic data. See ?AE98 details. comparison run-times, consider following three estimators nuisance parameters arising estimation local average treatment effect (LATE): Gradient boosting single base learner (see ?mdl_xgboost) Short-stacking linear regression (see ?ols), lasso (see ?mdl_glmnet), gradient boosting Stacking linear regression, lasso, gradient boosting stacking short-stacking construct weighted averages considered base learners minimize --sample mean squared prediction error (MSPE). difference two approaches lies construction MSPE: stacking runs cross-validation cross-fitting sample fold, short-stacking directly uses --sample predictions arising cross-fitting step double/debiased machine learning estimators. run-times show, results substantially reduced computational burden:","code":"# Load ddml and set seed library(ddml) set.seed(221945)  # Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")] time_singlelearner <- system.time({   late_fit <- ddml_late(y, D, Z, X,                         learners = list(what = mdl_xgboost,                                         args = list(nrounds = 100,                                                     max_depth = 1)),                         sample_folds = 10,                         silent = TRUE)   })#SYSTEM.TIME  time_shortstacking <- system.time({   late_fit <- ddml_late(y, D, Z, X,                         learners = list(list(fun = ols),                                         list(fun = mdl_glmnet),                                         list(fun = mdl_xgboost,                                              args = list(nrounds = 100,                                                          max_depth = 1))),                         ensemble_type = 'nnls1',                         shortstack = TRUE,                         sample_folds = 10,                         silent = TRUE)   })#SYSTEM.TIME  time_stacking <- system.time({   late_fit <- ddml_late(y, D, Z, X,                         learners = list(list(fun = ols),                                         list(fun = mdl_glmnet),                                         list(fun = mdl_xgboost,                                              args = list(nrounds = 100,                                                          max_depth = 1))),                         ensemble_type = 'nnls1',                         shortstack = FALSE,                         sample_folds = 10,                         cv_folds = 10,                         silent = TRUE)   })#SYSTEM.TIME cat(\"Time single learner:\", time_singlelearner[1], \"\\n\") #> Time single learner: 8.572 cat(\"Time short-stacking:\", time_shortstacking[1], \"\\n\") #> Time short-stacking: 20.102 cat(\"Time stacking:      \", time_stacking[1]) #> Time stacking:       119.961"},{"path":"https://www.thomaswiemann.com/ddml/articles/stacking.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Computational Benefits of Short-Stacking","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). “ddml: Double/debiased machine learning Stata.” https://arxiv.org/abs/2301.09397 Ahrens , Hansen C B, Schaffer M E, Wiemann T (2024). “Model averaging double machine learning.” https://arxiv.org/abs/2401.01645 Angrist J, Evans W (1998). “Children Parents’ Labor Supply: Evidence Exogenous Variation Family Size.” American Economic Review, 88(3), 450-477.","code":""},{"path":"https://www.thomaswiemann.com/ddml/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Achim Ahrens. Author. Christian B Hansen. Author. Mark E Schaffer. Author. Thomas Wiemann. Author, maintainer.","code":""},{"path":"https://www.thomaswiemann.com/ddml/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ahrens , Hansen C, Schaffer M, Wiemann T (2024). ddml: Double/Debiased Machine Learning. R package version 0.2.1.9000,  https://thomaswiemann.com/ddml/, https://github.com/thomaswiemann/ddml.","code":"@Manual{,   title = {ddml: Double/Debiased Machine Learning},   author = {Achim Ahrens and Christian B Hansen and Mark E Schaffer and Thomas Wiemann},   year = {2024},   note = {R package version 0.2.1.9000,  https://thomaswiemann.com/ddml/},   url = {https://github.com/thomaswiemann/ddml}, }"},{"path":"https://www.thomaswiemann.com/ddml/index.html","id":"ddml","dir":"","previous_headings":"","what":"Double/Debiased Machine Learning in R","title":"Double/Debiased Machine Learning in R","text":"ddml implementation double/debiased machine learning estimators proposed Chernozhukov et al. (2018). key feature ddml straightforward estimation nuisance parameters using (short-)stacking (Wolpert, 1992), allows multiple machine learners increase robustness underlying data generating process. See also Ahrens et al. (2024) detailed illustration practical benefits combining DDML (short-)stacking. ddml sister R package Stata package, mirroring key features also leveraging R simplify estimation user-provided machine learners /sparse matrices. See also Ahrens et al. (2023) additional discussion supported causal models benefits (short)-stacking.","code":""},{"path":"https://www.thomaswiemann.com/ddml/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Double/Debiased Machine Learning in R","text":"Install latest development version GitHub (requires devtools package): Install latest public release CRAN:","code":"if (!require(\"devtools\")) {   install.packages(\"devtools\") } devtools::install_github(\"thomaswiemann/ddml\", dependencies = TRUE) install.packages(\"ddml\")"},{"path":"https://www.thomaswiemann.com/ddml/index.html","id":"example-late-estimation-based-on-short-stacking","dir":"","previous_headings":"","what":"Example: LATE Estimation based on (Short-)Stacking","title":"Double/Debiased Machine Learning in R","text":"illustrate ddml simple example, consider included random subsample 5,000 observations data Angrist & Evans (1998). data contains information labor supply mothers, children, well demographic data. See ?AE98 details. ddml_late estimates local average treatment effect (LATE) using double/debiased machine learning (see ?ddml_late). Since statistical properties machine learners depend heavily underlying (unknown!) structure data, adaptive combination multiple machine learners can increase robustness. snippet, ddml_late estimates LATE short-stacking based three base learners: linear regression (see ?ols) lasso (see ?mdl_glmnet) gradient boosting (see ?mdl_xgboost)","code":"# Load ddml and set seed library(ddml) set.seed(75523)  # Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")] # Estimate the local average treatment effect using short-stacking with base #     learners ols, rlasso, and xgboost. late_fit_short <- ddml_late(y, D, Z, X,                             learners = list(list(fun = ols),                                             list(fun = mdl_glmnet),                                             list(fun = mdl_xgboost,                                                  args = list(nrounds = 100,                                                              max_depth = 1))),                             ensemble_type = 'nnls1',                             shortstack = TRUE,                             sample_folds = 10,                             silent = TRUE) summary(late_fit_short) #> LATE estimation results:  #>   #>         Estimate Std. Error   t value  Pr(>|t|) #> nnls1 -0.2105019   0.195529 -1.076576 0.2816698"},{"path":"https://www.thomaswiemann.com/ddml/index.html","id":"learn-more-about-ddml","dir":"","previous_headings":"","what":"Learn More about ddml","title":"Double/Debiased Machine Learning in R","text":"Check articles learn : vignette(\"ddml\") detailed introduction ddml vignette(\"stacking\") discusses computational benefits short-stacking vignette(\"new_ml_wrapper\") shows write user-provided base learners vignette(\"sparse\") illustrates support sparse matrices (see ?Matrix) vignette(\"\") discusses integration diff--diff package additional applied examples, see case studies: vignette(\"example_401k\") revisits effect 401k participation retirement savings vignette(\"example_BLP95\") considers flexible demand estimation endogenous prices","code":""},{"path":"https://www.thomaswiemann.com/ddml/index.html","id":"other-doubledebiased-machine-learning-packages","dir":"","previous_headings":"","what":"Other Double/Debiased Machine Learning Packages","title":"Double/Debiased Machine Learning in R","text":"ddml built easily (quickly) estimate common causal parameters multiple machine learners. support short-stacking, sparse matrices, easy--learn syntax, hope ddml useful complement DoubleML, expansive R Python package. DoubleML supports many advanced features multiway clustering stacking.","code":""},{"path":"https://www.thomaswiemann.com/ddml/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Double/Debiased Machine Learning in R","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). “ddml: Double/debiased machine learning Stata.” https://arxiv.org/abs/2301.09397 Ahrens , Hansen C B, Schaffer M E, Wiemann T (2024). “Model averaging double machine learning.” https://arxiv.org/abs/2401.01645 Angrist J, Evans W, (1998). “Children Parents’ Labor Supply: Evidence Exogenous Variation Family Size.” American Economic Review, 88(3), 450-477. Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). “Double/debiased machine learning treatment structural parameters.” Econometrics Journal, 21(1), C1-C68. Wolpert D H (1992). “Stacked generalization.” Neural Networks, 5(2), 241-259.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/AE98.html","id":null,"dir":"Reference","previous_headings":"","what":"Random subsample from the data of Angrist & Evans (1991). — AE98","title":"Random subsample from the data of Angrist & Evans (1991). — AE98","text":"Random subsample data Angrist & Evans (1991).","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/AE98.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random subsample from the data of Angrist & Evans (1991). — AE98","text":"","code":"AE98"},{"path":"https://www.thomaswiemann.com/ddml/reference/AE98.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Random subsample from the data of Angrist & Evans (1991). — AE98","text":"data frame 5,000 rows 13 variables. worked Indicator equal 1 mother employed. weeksw Number weeks employment. hoursw Hours worked per week. morekids Indicator equal 1 mother 2 kids. samesex Indicator equal 1 first two children sex. age Age years. agefst Age years birth first child. black Indicator equal 1 mother black. hisp Indicator equal 1 mother Hispanic. othrace Indicator equal 1 mother neither black Hispanic. educ Years education. boy1st Indicator equal 1 first child male. boy2nd Indicator equal 1 second child male.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/AE98.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Random subsample from the data of Angrist & Evans (1991). — AE98","text":"https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/11288","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/AE98.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Random subsample from the data of Angrist & Evans (1991). — AE98","text":"Angrist J, Evans W (1998). \"Children Parents' Labor Supply: Evidence Exogenous Variation Family Size.\" American Economic Review, 88(3), 450-477.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/crosspred.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Predictions using Stacking. — crosspred","title":"Cross-Predictions using Stacking. — crosspred","text":"Cross-predictions using stacking.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/crosspred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Predictions using Stacking. — crosspred","text":"","code":"crosspred(   y,   X,   Z = NULL,   learners,   sample_folds = 2,   ensemble_type = \"average\",   cv_folds = 5,   custom_ensemble_weights = NULL,   compute_insample_predictions = FALSE,   compute_predictions_bylearner = FALSE,   subsamples = NULL,   cv_subsamples_list = NULL,   silent = FALSE,   progress = NULL,   auxilliary_X = NULL )"},{"path":"https://www.thomaswiemann.com/ddml/reference/crosspred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Predictions using Stacking. — crosspred","text":"y outcome variable. X (sparse) matrix predictive variables. Z Optional additional (sparse) matrix predictive variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation predictor. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding predictive variables X passed base learner. assign_Z optional vector column indices corresponding predictive Z passed base learner. Omission args element results default arguments used fun. Omission assign_X (/assign_Z) results inclusion variables X (/Z). sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. cv_folds Number folds used cross-validation ensemble construction. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. compute_insample_predictions Indicator equal 1 -sample predictions also computed. compute_predictions_bylearner Indicator equal 1 -sample predictions also computed learner (rather entire ensemble). subsamples List vectors sample indices cross-fitting. cv_subsamples_list List lists, corresponding subsample containing vectors subsample indices cross-validation. silent Boolean silence estimation updates. progress String print learner cv fold progress. auxilliary_X optional list matrices length sample_folds, containing additional observations calculate predictions .","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/crosspred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Predictions using Stacking. — crosspred","text":"crosspred returns list containing following components: oos_fitted matrix --sample predictions, column corresponding ensemble type (chronological order). weights array, providing weight assigned base learner (chronological order) ensemble procedures. is_fitted compute_insample_predictions = T. list matrices -sample predictions sample fold. auxilliary_fitted auxilliary_X NULL, list matrices additional predictions. oos_fitted_bylearner compute_predictions_bylearner = T, matrix --sample predictions, column corresponding base learner (chronological order). is_fitted_bylearner compute_insample_predictions = T compute_predictions_bylearner = T, list matrices -sample predictions sample fold. auxilliary_fitted_bylearner auxilliary_X NULL compute_predictions_bylearner = T, list matrices additional predictions learner.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/crosspred.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cross-Predictions using Stacking. — crosspred","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/crosspred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Predictions using Stacking. — crosspred","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] X = AE98[, c(\"morekids\", \"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Compute cross-predictions using stacking with base learners ols and lasso. #     Two stacking approaches are simultaneously computed: Equally #     weighted (ensemble_type = \"average\") and MSPE-minimizing with weights #     in the unit simplex (ensemble_type = \"nnls1\"). Predictions for each #     learner are also calculated. crosspred_res <- crosspred(y, X,                            learners = list(list(fun = ols),                                            list(fun = mdl_glmnet)),                            ensemble_type = c(\"average\",                                              \"nnls1\",                                              \"singlebest\"),                            compute_predictions_bylearner = TRUE,                            sample_folds = 2,                            cv_folds = 2,                            silent = TRUE) dim(crosspred_res$oos_fitted) # = length(y) by length(ensemble_type) #> [1] 5000    3 dim(crosspred_res$oos_fitted_bylearner) # = length(y) by length(learners) #> [1] 5000    2"},{"path":"https://www.thomaswiemann.com/ddml/reference/crossval.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimator of the Mean Squared Prediction Error using Cross-Validation. — crossval","title":"Estimator of the Mean Squared Prediction Error using Cross-Validation. — crossval","text":"Estimator mean squared prediction error different learners using cross-validation.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/crossval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimator of the Mean Squared Prediction Error using Cross-Validation. — crossval","text":"","code":"crossval(   y,   X,   Z = NULL,   learners,   cv_folds = 5,   cv_subsamples = NULL,   silent = FALSE,   progress = NULL )"},{"path":"https://www.thomaswiemann.com/ddml/reference/crossval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimator of the Mean Squared Prediction Error using Cross-Validation. — crossval","text":"y outcome variable. X (sparse) matrix predictive variables. Z Optional additional (sparse) matrix predictive variables. learners learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding variables X passed base learner. assign_Z optional vector column indices corresponding variables Z passed base learner. Omission args element results default arguments used fun. Omission assign_X (/assign_Z) results inclusion predictive variables X (/Z). cv_folds Number folds used cross-validation. cv_subsamples List vectors sample indices cross-validation. silent Boolean silence estimation updates. progress String print learner cv fold progress.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/crossval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimator of the Mean Squared Prediction Error using Cross-Validation. — crossval","text":"crossval returns list containing following components: mspe vector MSPE estimates, corresponding base learners (chronological order). oos_resid matrix --sample prediction errors, column corresponding base learners (chronological order). cv_subsamples Pass-cv_subsamples. See .","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/crossval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimator of the Mean Squared Prediction Error using Cross-Validation. — crossval","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] X = AE98[, c(\"morekids\", \"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Compare ols, lasso, and ridge using 4-fold cross-validation cv_res <- crossval(y, X,                    learners = list(list(fun = ols),                                    list(fun = mdl_glmnet),                                    list(fun = mdl_glmnet,                                         args = list(alpha = 0))),                    cv_folds = 4,                    silent = TRUE) cv_res$mspe #> [1] 0.2378277 0.2365085 0.2365032"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml.html","id":null,"dir":"Reference","previous_headings":"","what":"ddml: Double/Debiased Machine Learning in R — ddml","title":"ddml: Double/Debiased Machine Learning in R — ddml","text":"Estimate common causal parameters using double/debiased machine learning proposed Chernozhukov et al. (2018). 'ddml' simplifies estimation based (short-)stacking, leverages multiple base learners increase robustness underlying data generating process.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ddml: Double/Debiased Machine Learning in R — ddml","text":"Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). \"Double/debiased machine learning treatment structural parameters.\" Econometrics Journal, 21(1), C1-C68.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimators of Average Treatment Effects. — ddml_ate","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"Estimators average treatment effect average treatment effect treated.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"","code":"ddml_ate(   y,   D,   X,   learners,   learners_DX = learners,   sample_folds = 2,   ensemble_type = \"nnls\",   shortstack = FALSE,   cv_folds = 5,   custom_ensemble_weights = NULL,   custom_ensemble_weights_DX = custom_ensemble_weights,   subsamples_D0 = NULL,   subsamples_D1 = NULL,   cv_subsamples_list_D0 = NULL,   cv_subsamples_list_D1 = NULL,   silent = FALSE )  ddml_att(   y,   D,   X,   learners,   learners_DX = learners,   sample_folds = 2,   ensemble_type = \"nnls\",   shortstack = FALSE,   cv_folds = 5,   custom_ensemble_weights = NULL,   custom_ensemble_weights_DX = custom_ensemble_weights,   subsamples_D0 = NULL,   subsamples_D1 = NULL,   cv_subsamples_list_D0 = NULL,   cv_subsamples_list_D1 = NULL,   silent = FALSE )"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"y outcome variable. D binary endogenous variable interest. X (sparse) matrix control variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation conditional expectation functions. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding control variables X passed base learner. Omission args element results default arguments used fun. Omission assign_X results inclusion variables X. learners_DX Optional argument allow different estimators \\(E[D|X]\\). Setup identical learners. sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. shortstack Boolean use short-stacking. cv_folds Number folds used cross-validation ensemble construction. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. custom_ensemble_weights_DX Optional argument allow different custom ensemble weights learners_DX. Setup identical custom_ensemble_weights. Note: custom_ensemble_weights custom_ensemble_weights_DX must number columns. subsamples_D0, subsamples_D1 List vectors sample indices cross-fitting, corresponding untreated treated observations, respectively. cv_subsamples_list_D0, cv_subsamples_list_D1 List lists, corresponding subsample containing vectors subsample indices cross-validation. Arguments separated untreated treated observations, respectively. silent Boolean silence estimation updates.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"ddml_ate ddml_att return object S3 class ddml_ate ddml_att, respectively. object class ddml_ate ddml_att list containing following components: ate / att vector average treatment effect / average treatment effect treated estimates. weights list matrices, providing weight assigned base learner (chronological order) ensemble procedure. mspe list matrices, providing MSPE base learner (chronological order) computed cross-validation step ensemble construction. psi_a, psi_b Matrices needed computation scores. Used summary.ddml_ate() summary.ddml_att(). learners,learners_DX, subsamples_D0,subsamples_D1, cv_subsamples_list_D0,cv_subsamples_list_D1, ensemble_type Pass-selected user-provided arguments. See .","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"ddml_ate ddml_att provide double/debiased machine learning  estimators average treatment effect average treatment effect treated, respectively, interactive model given \\(Y = g_0(D, X) + U,\\) \\((Y, D, X, U)\\) random vector \\(\\operatorname{supp} D = \\{0,1\\}\\), \\(E[U\\vert D, X] = 0\\), \\(\\Pr(D=1\\vert X) \\(0, 1)\\) probability 1, \\(g_0\\) unknown nuisance function. model, average treatment effect defined \\(\\theta_0^{\\textrm{ATE}} \\equiv E[g_0(1, X) - g_0(0, X)]\\). average treatment effect treated defined \\(\\theta_0^{\\textrm{ATT}} \\equiv E[g_0(1, X) - g_0(0, X)\\vert D = 1]\\).","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). \"Double/debiased machine learning treatment structural parameters.\" Econometrics Journal, 21(1), C1-C68. Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_ate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimators of Average Treatment Effects. — ddml_ate","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the average treatment effect using a single base learner, ridge. ate_fit <- ddml_ate(y, D, X,                     learners = list(what = mdl_glmnet,                                     args = list(alpha = 0)),                     sample_folds = 2,                     silent = TRUE) summary(ate_fit) #> ATE estimation results:  #>   #>    Estimate Std. Error   t value     Pr(>|t|) #>   -0.141632 0.01535324 -9.224892 2.838475e-20  # Estimate the average treatment effect using short-stacking with base #     learners ols, lasso, and ridge. We can also use custom_ensemble_weights #     to estimate the ATE using every individual base learner. weights_everylearner <- diag(1, 3) colnames(weights_everylearner) <- c(\"mdl:ols\", \"mdl:lasso\", \"mdl:ridge\") ate_fit <- ddml_ate(y, D, X,                     learners = list(list(fun = ols),                                     list(fun = mdl_glmnet),                                     list(fun = mdl_glmnet,                                          args = list(alpha = 0))),                     ensemble_type = 'nnls',                     custom_ensemble_weights = weights_everylearner,                     shortstack = TRUE,                     sample_folds = 2,                     silent = TRUE) summary(ate_fit) #> ATE estimation results:  #>   #>             Estimate Std. Error   t value     Pr(>|t|) #> nnls      -0.1433993 0.01526657 -9.393025 5.830105e-21 #> mdl:ols   -0.1362464 0.01593975 -8.547585 1.256911e-17 #> mdl:lasso -0.1464894 0.01533315 -9.553773 1.250572e-21 #> mdl:ridge -0.1468082 0.01528892 -9.602257 7.821283e-22"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"Estimator flexible partially linear IV model.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"","code":"ddml_fpliv(   y,   D,   Z,   X,   learners,   learners_DXZ = learners,   learners_DX = learners,   sample_folds = 2,   ensemble_type = \"nnls\",   shortstack = FALSE,   cv_folds = 5,   enforce_LIE = TRUE,   custom_ensemble_weights = NULL,   custom_ensemble_weights_DXZ = custom_ensemble_weights,   custom_ensemble_weights_DX = custom_ensemble_weights,   subsamples = NULL,   cv_subsamples_list = NULL,   silent = FALSE )"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"y outcome variable. D matrix endogenous variables. Z (sparse) matrix instruments. X (sparse) matrix control variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation conditional expectation functions. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding control variables X passed base learner. assign_Z optional vector column indices corresponding instruments Z passed base learner. Omission args element results default arguments used fun. Omission assign_X (/assign_Z) results inclusion variables X (/Z). learners_DXZ, learners_DX Optional arguments allow different estimators \\(E[D \\vert X, Z]\\), \\(E[D \\vert X]\\). Setup identical learners. sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. shortstack Boolean use short-stacking. cv_folds Number folds used cross-validation ensemble construction. enforce_LIE Indicator equal 1 law iterated expectations enforced first stage. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. custom_ensemble_weights_DXZ, custom_ensemble_weights_DX Optional arguments allow different custom ensemble weights learners_DXZ,learners_DX. Setup identical custom_ensemble_weights. Note: custom_ensemble_weights custom_ensemble_weights_DXZ,custom_ensemble_weights_DX must number columns. subsamples List vectors sample indices cross-fitting. cv_subsamples_list List lists, corresponding subsample containing vectors subsample indices cross-validation. silent Boolean silence estimation updates.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"ddml_fpliv returns object S3 class ddml_fpliv. object class ddml_fpliv list containing following components: coef vector \\(\\theta_0\\) estimates. weights list matrices, providing weight assigned base learner (chronological order) ensemble procedure. mspe list matrices, providing MSPE base learner (chronological order) computed cross-validation step ensemble construction. iv_fit Object class ivreg IV regression \\(Y - \\hat{E}[Y\\vert X]\\) \\(D - \\hat{E}[D\\vert X]\\) using \\(\\hat{E}[D\\vert X,Z] - \\hat{E}[D\\vert X]\\) instrument. learners,learners_DX,learners_DXZ, subsamples,cv_subsamples_list,ensemble_type Pass-selected user-provided arguments. See .","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"ddml_fpliv provides double/debiased machine learning estimator parameter interest \\(\\theta_0\\) partially linear IV model given \\(Y = \\theta_0D + g_0(X) + U,\\) \\((Y, D, X, Z, U)\\) random vector \\(E[U\\vert X, Z] = 0\\) \\(E[Var(E[D\\vert X, Z]\\vert X)] \\neq 0\\), \\(g_0\\) unknown nuisance function.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). \"Double/debiased machine learning treatment structural parameters.\" Econometrics Journal, 21(1), C1-C68. Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_fpliv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimator for the Flexible Partially Linear IV Model. — ddml_fpliv","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\", drop = FALSE] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the partially linear IV model using a single base learner: Ridge. fpliv_fit <- ddml_fpliv(y, D, Z, X,                         learners = list(what = mdl_glmnet,                                         args = list(alpha = 0)),                         sample_folds = 2,                         silent = TRUE) summary(fpliv_fit) #> FPLIV estimation results:  #>   #> , , single base learner #>  #>                  Estimate  Std. Error     t value  Pr(>|t|) #> (Intercept)  0.0006759649 0.006891024  0.09809354 0.9218580 #> D_r         -0.2066855916 0.202367083 -1.02133998 0.3070934 #>"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimator of the Local Average Treatment Effect. — ddml_late","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"Estimator local average treatment effect.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"","code":"ddml_late(   y,   D,   Z,   X,   learners,   learners_DXZ = learners,   learners_ZX = learners,   sample_folds = 2,   ensemble_type = \"nnls\",   shortstack = FALSE,   cv_folds = 5,   custom_ensemble_weights = NULL,   custom_ensemble_weights_DXZ = custom_ensemble_weights,   custom_ensemble_weights_ZX = custom_ensemble_weights,   subsamples_Z0 = NULL,   subsamples_Z1 = NULL,   cv_subsamples_list_Z0 = NULL,   cv_subsamples_list_Z1 = NULL,   silent = FALSE )"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"y outcome variable. D binary endogenous variable interest. Z Binary instrumental variable. X (sparse) matrix control variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation conditional expectation functions. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding control variables X passed base learner. assign_Z optional vector column indices corresponding instruments Z passed base learner. Omission args element results default arguments used fun. Omission assign_X (/assign_Z) results inclusion variables X (/Z). learners_DXZ, learners_ZX Optional arguments allow different estimators \\(E[D \\vert X, Z]\\), \\(E[Z \\vert X]\\). Setup identical learners. sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. shortstack Boolean use short-stacking. cv_folds Number folds used cross-validation ensemble construction. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. custom_ensemble_weights_DXZ, custom_ensemble_weights_ZX Optional arguments allow different custom ensemble weights learners_DXZ,learners_ZX. Setup identical custom_ensemble_weights. Note: custom_ensemble_weights custom_ensemble_weights_DXZ,custom_ensemble_weights_ZX must number columns. subsamples_Z0, subsamples_Z1 List vectors sample indices cross-fitting, corresponding observations \\(Z=0\\) \\(Z=1\\), respectively. cv_subsamples_list_Z0, cv_subsamples_list_Z1 List lists, corresponding subsample containing vectors subsample indices cross-validation. Arguments separated observations \\(Z=0\\) \\(Z=1\\), respectively. silent Boolean silence estimation updates.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"ddml_late returns object S3 class ddml_late. object class ddml_late list containing following components: late vector average treatment effect estimates. weights list matrices, providing weight assigned base learner (chronological order) ensemble procedure. mspe list matrices, providing MSPE base learner (chronological order) computed cross-validation step ensemble construction. psi_a, psi_b Matrices needed computation scores. Used summary.ddml_late(). learners,learners_DXZ,learners_ZX, subsamples_Z0,subsamples_Z1, cv_subsamples_list_Z0,cv_subsamples_list_Z1, ensemble_type Pass-selected user-provided arguments. See .","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"ddml_late provides double/debiased machine learning estimator local average treatment effect interactive model given \\(Y = g_0(D, X) + U,\\) \\((Y, D, X, Z, U)\\) random vector \\(\\operatorname{supp} D = \\operatorname{supp} Z = \\{0,1\\}\\), \\(E[U\\vert X, Z] = 0\\), \\(E[Var(E[D\\vert X, Z]\\vert X)] \\neq 0\\), \\(\\Pr(Z=1\\vert X) \\(0, 1)\\) probability 1, \\(p_0(1, X) \\geq p_0(0, X)\\) probability 1 \\(p_0(Z, X) \\equiv \\Pr(D=1\\vert Z, X)\\), \\(g_0\\) unknown nuisance function. model, local average treatment effect defined \\(\\theta_0^{\\textrm{LATE}} \\equiv     E[g_0(1, X) - g_0(0, X)\\vert p_0(1, X) > p(0, X)]\\).","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). \"Double/debiased machine learning treatment structural parameters.\" Econometrics Journal, 21(1), C1-C68. Imbens G, Angrist J (1004). \"Identification Estimation Local Average Treatment Effects.\" Econometrica, 62(2), 467-475. Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_late.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimator of the Local Average Treatment Effect. — ddml_late","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the local average treatment effect using a single base learner, #     ridge. late_fit <- ddml_late(y, D, Z, X,                       learners = list(what = mdl_glmnet,                                       args = list(alpha = 0)),                       sample_folds = 2,                       silent = TRUE) summary(late_fit) #> LATE estimation results:  #>   #>     Estimate Std. Error   t value  Pr(>|t|) #>   -0.2126393  0.1983906 -1.071822 0.2838002  # Estimate the local average treatment effect using short-stacking with base #     learners ols, lasso, and ridge. We can also use custom_ensemble_weights #     to estimate the ATE using every individual base learner. weights_everylearner <- diag(1, 3) colnames(weights_everylearner) <- c(\"mdl:ols\", \"mdl:lasso\", \"mdl:ridge\") late_fit <- ddml_late(y, D, Z, X,                       learners = list(list(fun = ols),                                       list(fun = mdl_glmnet),                                       list(fun = mdl_glmnet,                                            args = list(alpha = 0))),                       ensemble_type = 'nnls',                       custom_ensemble_weights = weights_everylearner,                       shortstack = TRUE,                       sample_folds = 2,                       silent = TRUE) summary(late_fit) #> LATE estimation results:  #>   #>              Estimate Std. Error    t value  Pr(>|t|) #> nnls      -0.21084693  0.1944477 -1.0843377 0.2782151 #> mdl:ols   -0.09830777  0.1702596 -0.5773992 0.5636698 #> mdl:lasso -0.18826158  0.1979579 -0.9510184 0.3415950 #> mdl:ridge -0.20961093  0.1979483 -1.0589175 0.2896373"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimator for the Partially Linear IV Model. — ddml_pliv","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"Estimator partially linear IV model.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"","code":"ddml_pliv(   y,   D,   Z,   X,   learners,   learners_DX = learners,   learners_ZX = learners,   sample_folds = 2,   ensemble_type = \"nnls\",   shortstack = FALSE,   cv_folds = 5,   custom_ensemble_weights = NULL,   custom_ensemble_weights_DX = custom_ensemble_weights,   custom_ensemble_weights_ZX = custom_ensemble_weights,   subsamples = NULL,   cv_subsamples_list = NULL,   silent = FALSE )"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"y outcome variable. D matrix endogenous variables. Z matrix instruments. X (sparse) matrix control variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation conditional expectation functions. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding control variables X passed base learner. assign_Z optional vector column indices corresponding instruments Z passed base learner. Omission args element results default arguments used fun. Omission assign_X (/assign_Z) results inclusion variables X (/Z). learners_DX, learners_ZX Optional arguments allow different base learners estimation \\(E[D|X]\\), \\(E[Z|X]\\). Setup identical learners. sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. shortstack Boolean use short-stacking. cv_folds Number folds used cross-validation ensemble construction. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. custom_ensemble_weights_DX, custom_ensemble_weights_ZX Optional arguments allow different custom ensemble weights learners_DX,learners_ZX. Setup identical custom_ensemble_weights. Note: custom_ensemble_weights custom_ensemble_weights_DX,custom_ensemble_weights_ZX must number columns. subsamples List vectors sample indices cross-fitting. cv_subsamples_list List lists, corresponding subsample containing vectors subsample indices cross-validation. silent Boolean silence estimation updates.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"ddml_pliv returns object S3 class ddml_pliv. object class ddml_pliv list containing following components: coef vector \\(\\theta_0\\) estimates. weights list matrices, providing weight assigned base learner (chronological order) ensemble procedure. mspe list matrices, providing MSPE base learner (chronological order) computed cross-validation step ensemble construction. iv_fit Object class ivreg IV regression \\(Y - \\hat{E}[Y\\vert X]\\) \\(D - \\hat{E}[D\\vert X]\\) using \\(Z - \\hat{E}[Z\\vert X]\\) instrument. See also AER::ivreg() details. learners,learners_DX,learners_ZX, subsamples,cv_subsamples_list,ensemble_type Pass-selected user-provided arguments. See .","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"ddml_pliv provides double/debiased machine learning estimator parameter interest \\(\\theta_0\\) partially linear IV model given \\(Y = \\theta_0D + g_0(X) + U,\\) \\((Y, D, X, Z, U)\\) random vector \\(E[Cov(U, Z\\vert X)] = 0\\) \\(E[Cov(D, Z\\vert X)] \\neq 0\\), \\(g_0\\) unknown nuisance function.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). \"Double/debiased machine learning treatment structural parameters.\" Econometrics Journal, 21(1), C1-C68. Kleiber C, Zeileis (2008). Applied Econometrics R. Springer-Verlag, New York. Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_pliv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimator for the Partially Linear IV Model. — ddml_pliv","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] Z = AE98[, \"samesex\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the partially linear IV model using a single base learner, ridge. pliv_fit <- ddml_pliv(y, D, Z, X,                       learners = list(what = mdl_glmnet,                                       args = list(alpha = 0)),                       sample_folds = 2,                       silent = TRUE) summary(pliv_fit) #> PLIV estimation results:  #>   #> , , single base learner #>  #>                  Estimate  Std. Error       t value  Pr(>|t|) #> (Intercept) -3.441427e-07 0.006902759 -4.985582e-05 0.9999602 #> D_r         -2.350584e-01 0.189341420 -1.241452e+00 0.2144387 #>"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimator for the Partially Linear Model. — ddml_plm","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"Estimator partially linear model.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"","code":"ddml_plm(   y,   D,   X,   learners,   learners_DX = learners,   sample_folds = 2,   ensemble_type = \"nnls\",   shortstack = FALSE,   cv_folds = 5,   custom_ensemble_weights = NULL,   custom_ensemble_weights_DX = custom_ensemble_weights,   subsamples = NULL,   cv_subsamples_list = NULL,   silent = FALSE )"},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"y outcome variable. D matrix endogenous variables. X (sparse) matrix control variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation conditional expectation functions. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding control variables X passed base learner. Omission args element results default arguments used fun. Omission assign_X results inclusion variables X. learners_DX Optional argument allow different estimators \\(E[D|X]\\). Setup identical learners. sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. shortstack Boolean use short-stacking. cv_folds Number folds used cross-validation ensemble construction. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. custom_ensemble_weights_DX Optional argument allow different custom ensemble weights learners_DX. Setup identical custom_ensemble_weights. Note: custom_ensemble_weights custom_ensemble_weights_DX must number columns. subsamples List vectors sample indices cross-fitting. cv_subsamples_list List lists, corresponding subsample containing vectors subsample indices cross-validation. silent Boolean silence estimation updates.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"ddml_plm returns object S3 class ddml_plm. object class ddml_plm list containing following components: coef vector \\(\\theta_0\\) estimates. weights list matrices, providing weight assigned base learner (chronological order) ensemble procedure. mspe list matrices, providing MSPE base learner (chronological order) computed cross-validation step ensemble construction. ols_fit Object class lm second stage regression \\(Y - \\hat{E}[Y|X]\\) \\(D - \\hat{E}[D|X]\\). learners,learners_DX,subsamples, cv_subsamples_list,ensemble_type Pass-selected user-provided arguments. See .","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"ddml_plm provides double/debiased machine learning estimator parameter interest \\(\\theta_0\\) partially linear model given \\(Y = \\theta_0D + g_0(X) + U,\\) \\((Y, D, X, U)\\) random vector \\(E[Cov(U, D\\vert X)] = 0\\) \\(E[Var(D\\vert X)] \\neq 0\\), \\(g_0\\) unknown nuisance function.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). \"Double/debiased machine learning treatment structural parameters.\" Econometrics Journal, 21(1), C1-C68. Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/ddml_plm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimator for the Partially Linear Model. — ddml_plm","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the partially linear model using a single base learner, ridge. plm_fit <- ddml_plm(y, D, X,                     learners = list(what = mdl_glmnet,                                     args = list(alpha = 0)),                     sample_folds = 2,                     silent = TRUE) summary(plm_fit) #> PLM estimation results:  #>   #> , , single base learner #>  #>                  Estimate  Std. Error     t value     Pr(>|t|) #> (Intercept)  0.0007204734 0.006877341   0.1047605 9.165659e-01 #> D_r         -0.1483220220 0.014700193 -10.0898010 6.129365e-24 #>   # Estimate the partially linear model using short-stacking with base learners #     ols, lasso, and ridge. We can also use custom_ensemble_weights #     to estimate the ATE using every individual base learner. weights_everylearner <- diag(1, 3) colnames(weights_everylearner) <- c(\"mdl:ols\", \"mdl:lasso\", \"mdl:ridge\") plm_fit <- ddml_plm(y, D, X,                     learners = list(list(fun = ols),                                     list(fun = mdl_glmnet),                                     list(fun = mdl_glmnet,                                          args = list(alpha = 0))),                     ensemble_type = 'nnls',                     custom_ensemble_weights = weights_everylearner,                     shortstack = TRUE,                     sample_folds = 2,                     silent = TRUE) summary(plm_fit) #> PLM estimation results:  #>   #> , , nnls #>  #>                Estimate  Std. Error    t value     Pr(>|t|) #> (Intercept)  0.00147421 0.006883107   0.214178 8.304082e-01 #> D_r         -0.14947020 0.014730261 -10.147152 3.411741e-24 #>  #> , , mdl:ols #>  #>                 Estimate  Std. Error   t value     Pr(>|t|) #> (Intercept)  0.003412073 0.006898066  0.494642 6.208528e-01 #> D_r         -0.141683435 0.014682123 -9.650064 4.912490e-22 #>  #> , , mdl:lasso #>  #>                  Estimate  Std. Error      t value     Pr(>|t|) #> (Intercept) -0.0002879934 0.006884257  -0.04183362 9.666313e-01 #> D_r         -0.1495940169 0.014729260 -10.15624781 3.108085e-24 #>  #> , , mdl:ridge #>  #>                  Estimate  Std. Error      t value     Pr(>|t|) #> (Intercept) -0.0002309068 0.006884361  -0.03354077 9.732434e-01 #> D_r         -0.1494407672 0.014732341 -10.14372166 3.533745e-24 #>"},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for stats::glm(). — mdl_glm","title":"Wrapper for stats::glm(). — mdl_glm","text":"Simple wrapper stats::glm().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for stats::glm(). — mdl_glm","text":"","code":"mdl_glm(y, X, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for stats::glm(). — mdl_glm","text":"y outcome variable. X feature matrix. ... Additional arguments passed glm. See stats::glm() complete list arguments.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper for stats::glm(). — mdl_glm","text":"mdl_glm returns object S3 class mdl_glm simple mask return object stats::glm().","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper for stats::glm(). — mdl_glm","text":"","code":"glm_fit <- mdl_glm(sample(0:1, 100, replace = TRUE),                    matrix(rnorm(1000), 100, 10)) class(glm_fit) #> [1] \"mdl_glm\" \"glm\"     \"lm\""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for glmnet::glmnet(). — mdl_glmnet","title":"Wrapper for glmnet::glmnet(). — mdl_glmnet","text":"Simple wrapper glmnet::glmnet() glmnet::cv.glmnet().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for glmnet::glmnet(). — mdl_glmnet","text":"","code":"mdl_glmnet(y, X, cv = TRUE, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for glmnet::glmnet(). — mdl_glmnet","text":"y outcome variable. X (sparse) feature matrix. cv Boolean indicate use lasso cross-validated penalty. ... Additional arguments passed glmnet. See glmnet::glmnet() glmnet::cv.glmnet() complete list arguments.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper for glmnet::glmnet(). — mdl_glmnet","text":"mdl_glmnet returns object S3 class mdl_glmnet simple mask return object glmnet::glmnet() glmnet::cv.glmnet().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glmnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wrapper for glmnet::glmnet(). — mdl_glmnet","text":"Friedman J, Hastie T, Tibshirani R (2010). \"Regularization Paths Generalized Linear Models via Coordinate Descent.\" Journal Statistical Software, 33(1), 1–22. Simon N, Friedman J, Hastie T, Tibshirani R (2011). \"Regularization Paths Cox's Proportional Hazards Model via Coordinate Descent.\" Journal Statistical Software, 39(5), 1–13.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_glmnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper for glmnet::glmnet(). — mdl_glmnet","text":"","code":"glmnet_fit <- mdl_glmnet(rnorm(100), matrix(rnorm(1000), 100, 10)) class(glmnet_fit) #> [1] \"mdl_glmnet\" \"cv.glmnet\""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_ranger.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for ranger::ranger(). — mdl_ranger","title":"Wrapper for ranger::ranger(). — mdl_ranger","text":"Simple wrapper ranger::ranger().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_ranger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for ranger::ranger(). — mdl_ranger","text":"","code":"mdl_ranger(y, X, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_ranger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for ranger::ranger(). — mdl_ranger","text":"y outcome variable. X feature matrix. ... Additional arguments passed ranger. See ranger::ranger() complete list arguments.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_ranger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper for ranger::ranger(). — mdl_ranger","text":"mdl_ranger returns object S3 class ranger simple mask return object ranger::ranger().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_ranger.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wrapper for ranger::ranger(). — mdl_ranger","text":"Wright M N, Ziegler (2017). \"ranger: fast implementation random forests high dimensional data C++ R.\" Journal Statistical Software 77(1), 1-17.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_ranger.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper for ranger::ranger(). — mdl_ranger","text":"","code":"ranger_fit <- mdl_ranger(rnorm(100), matrix(rnorm(1000), 100, 10)) class(ranger_fit) #> [1] \"mdl_ranger\" \"ranger\""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for xgboost::xgboost(). — mdl_xgboost","title":"Wrapper for xgboost::xgboost(). — mdl_xgboost","text":"Simple wrapper xgboost::xgboost() changes default arguments.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for xgboost::xgboost(). — mdl_xgboost","text":"","code":"mdl_xgboost(y, X, nrounds = 500, verbose = 0, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_xgboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for xgboost::xgboost(). — mdl_xgboost","text":"y outcome variable. X (sparse) feature matrix. nrounds max number boosting iterations. verbose 0, xgboost stay silent. 1, print information performance. 2, additional information printed . Note setting verbose > 0 automatically engages cb.print.evaluation(period=1) callback function. ... Additional arguments passed xgboost. See xgboost::xgboost() complete list arguments.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_xgboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper for xgboost::xgboost(). — mdl_xgboost","text":"mdl_xgboost returns object S3 class mdl_xgboost simple mask return object xgboost::xgboost().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_xgboost.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wrapper for xgboost::xgboost(). — mdl_xgboost","text":"Chen T, Guestrin C (2011). \"Xgboost: Scalable Tree Boosting System.\" Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining, 785–794.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/mdl_xgboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper for xgboost::xgboost(). — mdl_xgboost","text":"","code":"xgboost_fit <- mdl_xgboost(rnorm(50), matrix(rnorm(150), 50, 3),                            nrounds = 1) class(xgboost_fit) #> [1] \"mdl_xgboost\" \"xgb.Booster\""},{"path":"https://www.thomaswiemann.com/ddml/reference/ols.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordinary least squares. — ols","title":"Ordinary least squares. — ols","text":"Simple implementation ordinary least squares computes sparse feature matrices.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordinary least squares. — ols","text":"","code":"ols(y, X, const = FALSE, w = NULL)"},{"path":"https://www.thomaswiemann.com/ddml/reference/ols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordinary least squares. — ols","text":"y outcome variable. X feature matrix. const Boolean equal TRUE constant included. default FALSE w vector weights weighted least squares.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/ols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ordinary least squares. — ols","text":"ols returns object S3 class ols. object class ols list containing following components: coef vector regression coefficents. y, X, const, w Pass-user-provided arguments. See .","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/ols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ordinary least squares. — ols","text":"","code":"ols_fit <- ols(rnorm(100), cbind(rnorm(100), rnorm(100)), const = TRUE) ols_fit$coef #>             [,1] #> [1,]  0.03270868 #> [2,]  0.08204289 #> [3,] -0.05981607"},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_ate.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_ate","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_ate","text":"Inference methods treatment effect estimators.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_ate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_ate","text":"","code":"# S3 method for summary.ddml_ate print(x, ...)  # S3 method for summary.ddml_att print(x, ...)  # S3 method for summary.ddml_late print(x, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_ate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_ate","text":"x object class summary.ddml_ate, summary.ddml_att, ddml_late, returned summary.ddml_ate(), summary.ddml_att(), summary.ddml_late(), respectively. ... Currently unused.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_ate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_ate","text":"NULL.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_ate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_ate","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the average treatment effect using a single base learner, ridge. ate_fit <- ddml_ate(y, D, X,                     learners = list(what = mdl_glmnet,                                     args = list(alpha = 0)),                     sample_folds = 2,                     silent = TRUE) summary(ate_fit) #> ATE estimation results:  #>   #>   Estimate Std. Error   t value     Pr(>|t|) #>   -0.15689 0.02079818 -7.543449 4.577018e-14"},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_plm.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_fpliv","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_fpliv","text":"Inference methods treatment effect estimators.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_plm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_fpliv","text":"","code":"# S3 method for summary.ddml_fpliv print(x, ...)  # S3 method for summary.ddml_pliv print(x, ...)  # S3 method for summary.ddml_plm print(x, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_plm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_fpliv","text":"x object class summary.ddml_plm, summary.ddml_pliv, summary.ddml_fpliv, returned summary.ddml_plm(), summary.ddml_pliv(), summary.ddml_fpliv(), respectively. ... Currently unused.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_plm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_fpliv","text":"NULL.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/print.summary.ddml_plm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Methods for Treatment Effect Estimators. — print.summary.ddml_fpliv","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the partially linear model using a single base learner, ridge. plm_fit <- ddml_plm(y, D, X,                     learners = list(what = mdl_glmnet,                                     args = list(alpha = 0)),                     sample_folds = 2,                     silent = TRUE) summary(plm_fit) #> PLM estimation results:  #>   #> , , single base learner #>  #>                  Estimate  Std. Error     t value     Pr(>|t|) #> (Intercept) -0.0002638329 0.006891883 -0.03828169 9.694631e-01 #> D_r         -0.1459781440 0.014761572 -9.88906521 4.643434e-23 #>"},{"path":"https://www.thomaswiemann.com/ddml/reference/shortstacking.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictions using Short-Stacking. — shortstacking","title":"Predictions using Short-Stacking. — shortstacking","text":"Predictions using short-stacking.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/shortstacking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictions using Short-Stacking. — shortstacking","text":"","code":"shortstacking(   y,   X,   Z = NULL,   learners,   sample_folds = 2,   ensemble_type = \"average\",   custom_ensemble_weights = NULL,   compute_insample_predictions = FALSE,   subsamples = NULL,   silent = FALSE,   progress = NULL,   auxilliary_X = NULL,   shortstack_y = y )"},{"path":"https://www.thomaswiemann.com/ddml/reference/shortstacking.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictions using Short-Stacking. — shortstacking","text":"y outcome variable. X (sparse) matrix predictive variables. Z Optional additional (sparse) matrix predictive variables. learners May take one two forms, depending whether single learner stacking multiple learners used estimation predictor. single learner used, learners list two named elements: base learner function. function must predicts named input y using named input X. args Optional arguments passed . stacking multiple learners used, learners list lists, containing four named elements: fun base learner function. function must predicts named input y using named input X. args Optional arguments passed fun. assign_X optional vector column indices corresponding predictive variables X passed base learner. assign_Z optional vector column indices corresponding predictive Z passed base learner. Omission args element results default arguments used fun. Omission assign_X (/assign_Z) results inclusion variables X (/Z). sample_folds Number cross-fitting folds. ensemble_type Ensemble method combine base learners final estimate conditional expectation functions. Possible values : \"nnls\" Non-negative least squares. \"nnls1\" Non-negative least squares constraint weights sum one. \"singlebest\" Select base learner minimum MSPE. \"ols\" Ordinary least squares. \"average\" Simple average base learners. Multiple ensemble types may passed vector strings. custom_ensemble_weights numerical matrix user-specified ensemble weights. column corresponds custom ensemble specification, row corresponds base learner learners (chronological order). Optional column names used name estimation results corresponding custom ensemble specification. compute_insample_predictions Indicator equal 1 -sample predictions also computed. subsamples List vectors sample indices cross-fitting. silent Boolean silence estimation updates. progress String print learner cv fold progress. auxilliary_X optional list matrices length sample_folds, containing additional observations calculate predictions . shortstack_y Optional vector outcome variable form short-stacking predictions . Base learners always trained y.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/shortstacking.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predictions using Short-Stacking. — shortstacking","text":"shortstack returns list containing following components: oos_fitted matrix --sample predictions, column corresponding ensemble type (chronological order). weights array, providing weight assigned base learner (chronological order) ensemble procedures. is_fitted compute_insample_predictions = T. list matrices -sample predictions sample fold. auxilliary_fitted auxilliary_X NULL, list matrices additional predictions. oos_fitted_bylearner matrix --sample predictions, column corresponding base learner (chronological order). is_fitted_bylearner compute_insample_predictions = T, list matrices -sample predictions sample fold. auxilliary_fitted_bylearner auxilliary_X NULL, list matrices additional predictions learner. Note unlike crosspred, shortstack always computes --sample predictions base learner (additional computational cost).","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/shortstacking.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predictions using Short-Stacking. — shortstacking","text":"Ahrens , Hansen C B, Schaffer M E, Wiemann T (2023). \"ddml: Double/debiased machine learning Stata.\" https://arxiv.org/abs/2301.09397 Wolpert D H (1992). \"Stacked generalization.\" Neural Networks, 5(2), 241-259.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/shortstacking.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predictions using Short-Stacking. — shortstacking","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] X = AE98[, c(\"morekids\", \"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Compute predictions using shortstacking with base learners ols and lasso. #     Two stacking approaches are simultaneously computed: Equally #     weighted (ensemble_type = \"average\") and MSPE-minimizing with weights #     in the unit simplex (ensemble_type = \"nnls1\"). Predictions for each #     learner are also calculated. shortstack_res <- shortstacking(y, X,                                 learners = list(list(fun = ols),                                                 list(fun = mdl_glmnet)),                                 ensemble_type = c(\"average\",                                                   \"nnls1\",                                                   \"singlebest\"),                                 sample_folds = 2,                                 silent = TRUE) dim(shortstack_res$oos_fitted) # = length(y) by length(ensemble_type) #> [1] 5000    3 dim(shortstack_res$oos_fitted_bylearner) # = length(y) by length(learners) #> [1] 5000    2"},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_ate.html","id":null,"dir":"Reference","previous_headings":"","what":"Inference Methods for Treatment Effect Estimators. — summary.ddml_ate","title":"Inference Methods for Treatment Effect Estimators. — summary.ddml_ate","text":"Inference methods treatment effect estimators.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_ate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inference Methods for Treatment Effect Estimators. — summary.ddml_ate","text":"","code":"# S3 method for ddml_ate summary(object, ...)  # S3 method for ddml_att summary(object, ...)  # S3 method for ddml_late summary(object, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_ate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inference Methods for Treatment Effect Estimators. — summary.ddml_ate","text":"object object class ddml_ate, ddml_att, ddml_late, fitted ddml_ate(), ddml_att(), ddml_late(), respectively. ... Currently unused.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_ate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inference Methods for Treatment Effect Estimators. — summary.ddml_ate","text":"matrix inference results.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_ate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inference Methods for Treatment Effect Estimators. — summary.ddml_ate","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the average treatment effect using a single base learner, ridge. ate_fit <- ddml_ate(y, D, X,                     learners = list(what = mdl_glmnet,                                     args = list(alpha = 0)),                     sample_folds = 2,                     silent = TRUE) summary(ate_fit) #> ATE estimation results:  #>   #>     Estimate Std. Error  t value    Pr(>|t|) #>   -0.1461308 0.01529043 -9.55701 1.21208e-21"},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_plm.html","id":null,"dir":"Reference","previous_headings":"","what":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","title":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","text":"Inference methods partially linear estimators. Simple wrapper sandwich::vcovHC().","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_plm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","text":"","code":"# S3 method for ddml_fpliv summary(object, ...)  # S3 method for ddml_pliv summary(object, ...)  # S3 method for ddml_plm summary(object, ...)"},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_plm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","text":"object object class ddml_plm, ddml_pliv, ddml_fpliv fitted ddml_plm(), ddml_pliv(), ddml_fpliv(), respectively. ... Additional arguments passed vcovHC. See sandwich::vcovHC() complete list arguments.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_plm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","text":"array inference results ensemble_type.","code":""},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_plm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","text":"Zeileis (2004). \"Econometric Computing HC HAC Covariance Matrix Estimators.” Journal Statistical Software, 11(10), 1-17. Zeileis (2006). “Object-Oriented Computation Sandwich Estimators.” Journal Statistical Software, 16(9), 1-16. Zeileis , Köll S, Graham N (2020). “Various Versatile Variances: Object-Oriented Implementation Clustered Covariances R.” Journal Statistical Software, 95(1), 1-36.","code":""},{"path":[]},{"path":"https://www.thomaswiemann.com/ddml/reference/summary.ddml_plm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inference Methods for Partially Linear Estimators. — summary.ddml_fpliv","text":"","code":"# Construct variables from the included Angrist & Evans (1998) data y = AE98[, \"worked\"] D = AE98[, \"morekids\"] X = AE98[, c(\"age\",\"agefst\",\"black\",\"hisp\",\"othrace\",\"educ\")]  # Estimate the partially linear model using a single base learner, ridge. plm_fit <- ddml_plm(y, D, X,                     learners = list(what = mdl_glmnet,                                     args = list(alpha = 0)),                     sample_folds = 2,                     silent = TRUE) summary(plm_fit) #> PLM estimation results:  #>   #> , , single base learner #>  #>                  Estimate  Std. Error      t value     Pr(>|t|) #> (Intercept) -0.0001089781 0.006887432  -0.01582275 9.873758e-01 #> D_r         -0.1495605999 0.014731184 -10.15265314 3.224752e-24 #>"},{"path":"https://www.thomaswiemann.com/ddml/news/index.html","id":"ddml-development-version","dir":"Changelog","previous_headings":"","what":"ddml (development version)","title":"ddml (development version)","text":"Fixes output ddml::print.summary.ddml_plm ddml::print.summary.ddml_ate (#57).","code":""},{"path":"https://www.thomaswiemann.com/ddml/news/index.html","id":"ddml-021","dir":"Changelog","previous_headings":"","what":"ddml 0.2.1","title":"ddml 0.2.1","text":"CRAN release: 2024-05-26 Fixes permuted residuals returned ddml::crossval (#54).","code":""},{"path":"https://www.thomaswiemann.com/ddml/news/index.html","id":"ddml-020","dir":"Changelog","previous_headings":"","what":"ddml 0.2.0","title":"ddml 0.2.0","text":"CRAN release: 2024-01-09 Adds support average treatment effect treated estimator. Adds support local average treatment effect estimation perfect compliance perfect non-compliance. Adds support custom ensemble weights. Adds article integration package. Adds ddml::mdl_glm wrapper stats::glm().","code":""},{"path":"https://www.thomaswiemann.com/ddml/news/index.html","id":"ddml-010","dir":"Changelog","previous_headings":"","what":"ddml 0.1.0","title":"ddml 0.1.0","text":"CRAN release: 2023-08-29 Initial CRAN submission.","code":""}]
