---
title: "Example on the Effect of 401k Participation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example on the Effect of 401k Participation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = requireNamespace("readstata13")
)
```


# Introduction

This article illustrates key features of ``ddml`` using : The causal effect of 401k participation on retirement savings. In particular, the example showcases the following features:

- Estimation using a single machine learner
- Estimation using multiple machine learners via short-stacking
- Estimation using multiple machine learners _and_ different sets of control variables
- Estimation using different sets of machine learners for continuous and binary outcome and treatment variables

After introducing the data, the article illustrates each above feature in turn.

```{r setup}
library(ddml)
set.seed(2410072)
```

# Data construction

401K data from the Survey of Income and Program Participation (SIPP) from the 
year 1991. Data taken from the application in 
Chernozhukov et al. (2018). [link](https://academic.oup.com/ectj/article/21/1/C1/5056401s})

```{r}
library(readstata13)
SIPP91 <- read.dta13("data/sipp1991.dta")
```


```{r}
nobs <- nrow(SIPP91)
y <- as.matrix(SIPP91$net_tfa)
D <- as.matrix(SIPP91$p401)
X <- as.matrix(SIPP91[, c("age", "inc", "educ", "fsize", 
                      "marr", "twoearn", "db", "pira", "hown")])
```
# Estimation using a Single Machine Learner

```{r}

lm_fit <- lm(y ~ D + X)
round(summary(lm_fit)$coefficients[2, ], 2)
```

```{r}

xgboost_fit <- ddml_plm(y = y, D = D, X = X,
                        learners = list(what = mdl_xgboost,
                                        args = list(nrounds = 300)),
                        sample_folds = 10,
                        silent = F)
round(summary(xgboost_fit)[2, , 1], 2)
```
# Estimation using Multiple Machine Learners

```{r}

learners <- list(list(fun = ols),
                 list(fun = mdl_glmnet),
                 list(fun = mdl_ranger,
                      args = list(num.trees = 250,
                                  max.depth = 4)),
                 list(fun = mdl_ranger,
                      args = list(num.trees = 250,
                                  max.depth = 12)),
                 list(fun = mdl_xgboost,
                      args = list(nrounds = 100)),
                 list(fun = mdl_xgboost,
                      args = list(nrounds = 300)))

stacking_fit <- ddml_plm(y = y, D = D, X = X,
                         learners = learners,
                         ensemble_type = "nnls",
                         sample_folds = 10,
                         shortstack = T,
                         silent = F)
```

```{r}
round(summary(stacking_fit)[2, , 1], 2)

sapply(stacking_fit$weights, round, 4)

```

# Estimation with Different Sets of Control Variables

```{r}
X_series <- poly(X[, c("age", "inc", "educ", "fsize")], degree = 3)
X_c <- cbind(X, X_series)
X_baseline <- 1:dim(X)[2] # baseline variables and indicators
X_extended <- 5:dim(X_c)[2] # indicators & series expansion
```

```{r}
learners <- list(list(fun = ols,
                      assign_X = X_baseline),
                 list(fun = ols,
                      assign_X = X_extended),
                 list(fun = mdl_glmnet,
                      assign_X = X_extended),
                 list(fun = mdl_ranger,
                      args = list(num.trees = 250,
                                  max.depth = 4),
                      assign_X = X_baseline),
                 list(fun = mdl_ranger,
                      args = list(num.trees = 250,
                                  max.depth = 12),
                      assign_X = X_baseline),
                 list(fun = mdl_xgboost,
                      args = list(nrounds = 100),
                      assign_X = X_baseline),
                 list(fun = mdl_xgboost,
                      args = list(nrounds = 300),
                      assign_X = X_baseline))

stacking_fit <- ddml_plm(y = y, D = D, X = X_c,
                         learners = learners,
                         ensemble_type = "nnls",
                         sample_folds = 10,
                         shortstack = T,
                         silent = F)

```

```{r}
round(summary(stacking_fit)[2, , 1], 2)

sapply(stacking_fit$weights, round, 4)

```

# Estimation with Continious and Binary Outcome and Treatment Variables

```{r}
# Write glm wrapper
mdl_glm <- function(y, X, ...) {
  df <- data.frame(y, X) # transform data from matrices to data.frame
  glm_fit <- glm(y ~ ., data = df, ...) # fit glm
  class(glm_fit) <- c("mdl_glm", class(glm_fit)) # append class
  return(glm_fit) # return fitted glm object
}#MDL_GLM

# Write predict.glm wrapper
predict.mdl_glm <- function(object, newdata) {
  df <- data.frame(newdata) # transform data from matrices to data.frame
  predict.glm(object, df, type = "response")
}#PREDICT.MDL_GLM
```


```{r}
learners_DX <- list(list(fun = mdl_glm,
                         args = list(family = "binomial"),
                      assign_X = X_baseline),
                 list(fun = mdl_glm,
                         args = list(family = "binomial"),
                      assign_X = X_extended),
                 list(fun = mdl_glmnet,
                      args = list(family = "binomial"),
                      assign_X = X_extended),
                 list(fun = mdl_ranger,
                      args = list(num.trees = 250,
                                  max.depth = 4),
                      assign_X = X_baseline),
                 list(fun = mdl_ranger,
                      args = list(num.trees = 250,
                                  max.depth = 12),
                      assign_X = X_baseline),
                 list(fun = mdl_xgboost,
                      args = list(nrounds = 100),
                      assign_X = X_baseline),
                 list(fun = mdl_xgboost,
                      args = list(nrounds = 300),
                      assign_X = X_baseline))

stacking_fit <- ddml_plm(y = y, D = D, X = X_c,
                         learners = learners,
                         learners_DX = learners_DX,
                         ensemble_type = "nnls",
                         sample_folds = 10,
                         shortstack = T,
                         silent = F)

```

```{r}
round(summary(stacking_fit)[2, , 1], 2)

sapply(stacking_fit$weights, round, 4)

```

# Bonus: Instrumental Variable Estimation

```{r}

Z <- as.matrix(SIPP91$e401)

stacking_IV_fit <- ddml_pliv(y = y, D = D, Z = Z, X = X_c,
                         learners = learners,
                         learners_DX = learners_DX,
                         learners_ZX = learners_DX,
                         ensemble_type = "nnls",
                         sample_folds = 10,
                         shortstack = T,
                         silent = F)

```

```{r}
round(summary(stacking_IV_fit)[2, , 1], 2)

sapply(stacking_IV_fit$weights, round, 4)

```

# References

Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W, Robins J (2018). "Double/debiased machine learning for treatment and structural parameters." The Econometrics Journal, 21(1), C1-C68.

