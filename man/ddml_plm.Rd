% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ddml_plm.R
\name{ddml_plm}
\alias{ddml_plm}
\title{Estimator for the Partially Linear Model.}
\usage{
ddml_plm(
  y,
  D,
  X,
  learners,
  learners_DX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  subsamples = NULL,
  cv_subsamples_list = NULL,
  silent = FALSE
)
}
\arguments{
\item{y}{The outcome variable.}

\item{D}{The endogenous variable.}

\item{X}{A (sparse) matrix of control variables.}

\item{learners}{May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
conditional expectation functions.
If a single learner is used, \code{learners} is a list with two named
elements:
\itemize{
\item{\code{what} The base learner function. The function must be
such that it predicts a named input \code{y} using a named input
\code{X}.}
\item{\code{args} Optional arguments to be passed to \code{what}.}
}
If stacking with multiple learners is used, \code{learners} is a list of
lists, each containing four named elements:
\itemize{
\item{\code{fun} The base learner function. The function must be
such that it predicts a named input \code{y} using a named input
\code{X}.}
\item{\code{args} Optional arguments to be passed to \code{fun}.}
\item{\code{assign_X} An optional vector of indices corresponding to
features in \code{X} that are passed to the corresponding base
learner.}
}}

\item{learners_DX}{Optional argument to allow for different estimators of
\eqn{E[D|X]}. Setup is identical to \code{learners}.}

\item{sample_folds}{Number of cross-fitting folds.}

\item{ensemble_type}{Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
\itemize{
\item{\code{"nnls"} Non-negative least squares.}
\item{\code{"nnls1"} Non-negative least squares with the constraint
that all weights sum to one.}
\item{\code{"singlebest"} Select base learner with minimum MSPE.}
\item{\code{"ols"} Ordinary least squares.}
\item{\code{"average"} Simple average over base learners.}
}
Multiple ensemble types may be passed as a vector of strings.}

\item{shortstack}{Boolean to use short-stacking.}

\item{cv_folds}{Number of folds used for cross-validation in ensemble
construction.}

\item{subsamples}{List of vectors with sample indices for cross-fitting.}

\item{cv_subsamples_list}{List of lists, each corresponding to a subsample
containing vectors with vectors subsample indices for cross-validation.}

\item{silent}{Boolean to silence estimation updates.}
}
\value{
\code{ddml_plm} returns an object of S3 class
\code{ddml_plm}. An object of class \code{ddml_plm} is a list containing
the following components:
\describe{
\item{\code{coef}}{A vector with the \eqn{\theta_0} estimates.}
\item{\code{weights}}{A list of matrices, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedure.}
\item{\code{mspe}}{A list of matrices, providing the MSPE of each
base learner (in chronological order) computed by the
cross-validation step in the ensemble construction.}
\item{\code{ols_fit}}{Object of class \code{lm} from the second
stage regression of \eqn{Y - \hat{E}[Y|X]} on
\eqn{D - \hat{E}[D|X]}.}
\item{\code{learners},\code{learners_DX},\code{subsamples},
\code{cv_subsamples_list},\code{ensemble_type}}{Pass-through of
selected user-provided arguments. See above.}
}
}
\description{
Estimator for the partially linear model.
}
\details{
\code{ddml_plm} provides a double/debiased machine learning
estimator for the parameter of interest \eqn{\theta_0} in the partially
linear model given by

\eqn{Y = \theta_0D + g_0(X) + U,}

where \eqn{(Y, D, X, U)} is a random vector such that
\eqn{E[Cov(U, D\vert X)] = 0} and \eqn{E[Var(D\vert X)] \neq 0}, and
\eqn{g_0} is an unknown nuisance function.
}
\examples{
# Construct data from the included SIPP_1991 data
y = as.matrix(SIPP_1991$net_tfa)
D = as.matrix(SIPP_1991$p401)
X = as.matrix(SIPP_1991[, c("age", "inc", "educ", "fsize",
                            "marr", "twoearn", "db", "pira", "hown")])

# Estimate the partially linear model using a single base learner: Ridge.
plm_fit <- ddml_plm(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
plm_fit$coef

# Estimate the partially linear model using short-stacking with base learners
#     ols, rlasso, and xgboost.
plm_fit <- ddml_plm(y, D, X,
                    learners = list(list(fun = ols),
                                    list(fun = mdl_glmnet),
                                    list(fun = mdl_xgboost,
                                         args = list(nrounds = 300,
                                                     max_depth = 3))),
                    ensemble_type = 'nnls',
                    shortstack = TRUE,
                    sample_folds = 2,
                    silent = TRUE)
plm_fit$coef
}
\seealso{
Other ddml: 
\code{\link{ddml_ate}()},
\code{\link{ddml_fpliv}()},
\code{\link{ddml_late}()},
\code{\link{ddml_pliv}()}
}
\concept{ddml}
